<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Ricardo's Place</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2018-05-03T12:14:16-04:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Ricardo de Azambuja</name>
   <email>ricardo.azambuja@gmail.com</email>
 </author>

 
 <entry>
   <title>3D Printer Bed Leveling - The Blu-Tack way!</title>
   <link href="http://localhost:4000/3d_printing/2017/04/16/bed_levelling_blu-tack/"/>
   <updated>2017-04-16T00:00:00-04:00</updated>
   <id>http://localhost:4000/3d_printing/2017/04/16/bed_levelling_blu-tack</id>
   <content type="html">&lt;p&gt;Last month, I decided to sell my &lt;em&gt;good ol’ 3D printer&lt;/em&gt;, a HobbyKing incarnation (&lt;a href=&quot;https://www.youtube.com/watch?v=EIGfol2M1sI&quot;&gt;Turnigy Fabrikator Mini&lt;/a&gt;) of the &lt;a href=&quot;http://www.tinyboy.net/&quot;&gt;TinyBoy V1&lt;/a&gt;, to buy a &lt;a href=&quot;http://reprap.org/wiki/Delta_geometry&quot;&gt;delta&lt;/a&gt; one.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/infinite_3d_print.gif?style=centerme&quot; alt=&quot;Linear Delta 3D Printer&quot; /&gt;
  &lt;figcaption&gt;My new Linear Delta 3D Printer working and working and working...&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;My old printer was one of the cheapest printers when I bought it (end of 2015), fully assembled and from a well-known supplier (something good when you are starting at 3D printing). I really liked my first 3D printer, but its printing volume was too small and the printer was a little bit too slow. I had upgraded it with an &lt;a href=&quot;http://www.thingiverse.com/thing:1036284&quot;&gt;extra fan&lt;/a&gt;, &lt;a href=&quot;http://www.thingiverse.com/thing:957550&quot;&gt;spool holder&lt;/a&gt;, removable bed (no pictures or links, sorry), and a &lt;a href=&quot;https://www.raspberrypi.org/&quot;&gt;Raspberry Pi&lt;/a&gt; running &lt;a href=&quot;http://octoprint.org/&quot;&gt;Octoprint&lt;/a&gt; before I sold it to my friend &lt;a href=&quot;https://www.researchgate.net/profile/Frederico_Belmonte_Klein&quot;&gt;Frederico&lt;/a&gt;. However, I’ve just realized I don’t have any pictures of my old 3D printer after my upgrades and the only picture I could find (&lt;a href=&quot;https://photos.google.com/&quot;&gt;Google Photos&lt;/a&gt; is really, really bad at searching for things and it could not even find an album after searching for its name!) was this one:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/my_first_3d_printer.jpg?style=centerme&quot; alt=&quot;My first 3D printer&quot; /&gt;
  &lt;figcaption&gt;The only photograph I could find of my old 3D printer.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So, I’ve been using 3D printers since the end of 2015, when I got my TinyBoy V1, and I decided that I should go for a kit and start doing more serious &lt;a href=&quot;https://en.wikipedia.org/wiki/Hacker_culture&quot;&gt;hacking&lt;/a&gt;. I searched online for a nice and cheap kit, but one at least faster and bigger than my old printer. Additionally, I was looking for something with a small footprint because space is luxury at home. After analyzing a lot of options, I finally decided to buy a &lt;a href=&quot;http://reprap.org/wiki/Kossel&quot;&gt;linear delta 3D printer&lt;/a&gt; instead of a &lt;a href=&quot;http://reprap.org/wiki/Prusa_i3&quot;&gt;classical cartesian one&lt;/a&gt;. The kit I got was from Anycubic (around 200 pounds at Amazon - April 2017), just like the one &lt;a href=&quot;https://www.youtube.com/watch?v=Bd7Z4JIQjQM&quot;&gt;from this video&lt;/a&gt;. Here, I will let my advice: &lt;em&gt;if you don’t like to play around with nuts, bolts and things that don’t seem to fit together at a first look (and sometimes at a second, third…:confused:), buy an assembled printer&lt;/em&gt;. Delta printers are faster than cartesian ones because they only need to move around its hot end (normally a &lt;a href=&quot;http://www.fabbaloo.com/blog/2015/11/11/bowden-or-direct-a-primer-on-extruder-styles&quot;&gt;bowden extruder&lt;/a&gt;). Nonetheless, delta printers need a more complicated &lt;a href=&quot;http://reprap.org/wiki/Delta_geometry&quot;&gt;modeling&lt;/a&gt; (not &lt;strong&gt;so&lt;/strong&gt; complicated if it’s a linear delta printer) to convert the steps from its three stepper motors into precise hot end movements. Actually, what I keep calling here a (linear) delta printer is a type of &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_manipulator&quot;&gt;parallel manipulator&lt;/a&gt; driven by linear actuators (remember, the circular movement of the stepper motors is converted into a linear movement).&lt;/p&gt;

&lt;p&gt;The cheap Anycubic printer I got doesn’t have the automatic leveling sensor since the manufacturer says bed leveling takes too long and it doesn’t work so well :unamused:. Ok, they got a point as the current version of the &lt;a href=&quot;https://github.com/MarlinFirmware/Marlin&quot;&gt;Marlin firmware&lt;/a&gt; (1.1.0-RC8 - 6 Dec 2016) doesn’t allow the user to save to &lt;a href=&quot;https://en.wikipedia.org/wiki/EPROM&quot;&gt;EPROM&lt;/a&gt; those values and, every time the printer performs the &lt;a href=&quot;https://en.wikipedia.org/wiki/Homing&quot;&gt;homing&lt;/a&gt;, it loses the bed leveling calibration. One possible solution to this problem is leveling the bed manually using one of the many adaptors available (&lt;a href=&quot;http://www.thingiverse.com/thing:220874/#remixes&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;http://www.thingiverse.com/thing:1960609&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;http://www.thingiverse.com/thing:2092046&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://www.thingiverse.com/thing:1732494&quot;&gt;that&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The only problem is: how can you print one of the adaptors I’ve presented above if your printer does not work properly yet? My solution was &lt;a href=&quot;https://en.wikipedia.org/wiki/Blu-Tack&quot;&gt;Blu-Tack&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Of course, I tried to print something before using the &lt;em&gt;Blu-Tack technique&lt;/em&gt;, but, as you can see in the picture below, one side was almost scrubbing the nozzle against the bed while the other was floating.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/bad_bed_leveling.jpg?style=centerme&quot; alt=&quot;Bed leveling&quot; /&gt;
  &lt;figcaption&gt;It is very hard to print if the bed leveling is so bad that the PLA doesn't adhere (top of the defective circle).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The solution to my problem started by connecting into my 3D printer using &lt;a href=&quot;https://github.com/kliment/Printrun&quot;&gt;Printrun Proterface&lt;/a&gt;, running the homing procedure for all axis (just click on the grey icon with a house) and lowering the hot end until it was close to touch the bed using a sequence of &lt;a href=&quot;https://en.wikipedia.org/wiki/G-code&quot;&gt;G-codes&lt;/a&gt; like this: &lt;code class=&quot;highlighter-rouge&quot;&gt;G1Z100, G1Z50, G1Z10, G1Z1, G1Z0.5&lt;/code&gt;.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/pronterface.png?style=centerme&quot; alt=&quot;Printrun Proterface&quot; /&gt;
  &lt;figcaption&gt;Printrun Proterface running on my computer.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Please, be careful:&lt;/strong&gt; if your firmware (Marlin) has a crazy value for &lt;code class=&quot;highlighter-rouge&quot;&gt;MANUAL_Z_HOME_POS&lt;/code&gt; (inside Marlin Arduino sketch, file Configuration.h), or even maybe if you forgot to do the homing, the G-codes I’ve just passed may damage your printer :fearful:. Also, if &lt;code class=&quot;highlighter-rouge&quot;&gt;min_software_endstops&lt;/code&gt; is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; you will not be able to drive negative values for Z like &lt;code class=&quot;highlighter-rouge&quot;&gt;G1Z-0.6&lt;/code&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/max_z_home.png?style=centerme&quot; alt=&quot;MANUAL_Z_HOME_POS&quot; /&gt;
  &lt;figcaption&gt;Arduino IDE with the Marlin firmware sketch.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After that, I used the &lt;em&gt;famous&lt;/em&gt; &lt;a href=&quot;http://reprap.org/wiki/Calibration#Bed_Leveling&quot;&gt;piece of paper method&lt;/a&gt; and moved the hot end around to verify the distances noticing the directions where the distance was increasing / decreasing.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/piece_of_paper_3d_printer_calibration.jpg?style=centerme&quot; alt=&quot;Bed leveling with paper sheet&quot; /&gt;
  &lt;figcaption&gt;Try to pull the paper, if it offers some resistance, you have found the secret Z value!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It’s important to highlight some details about delta printer calibration. Usually, you need to correct, via firmware, problems related with a &lt;a href=&quot;http://ladvien.github.io/robots/kossel-mini-calibration/&quot;&gt;convex/concave movement&lt;/a&gt; pattern and these problems are more directly connected to printer inner part dimensions. Here, I’m just leveling the bed.&lt;/p&gt;

&lt;p&gt;Knowing where the hot end was rising in relation to the bed after the paper test, I added little balls of Blu-Tack sized accordingly to those visual measurements.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/blu_tack_bed_leveling.jpg?style=centerme&quot; alt=&quot;Bed leveling with Blu-Tack&quot; /&gt;
  &lt;figcaption&gt;3D printer bed leveling: Blu-Tack style!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I put the 3D printer bed back and gently (maybe not so gently) pressed it against the Blue-Tack. The idea is to use the hot end to level the surface, but to avoid losing steps, or damaging the nozzle, it is necessary to roughly adjust the heights beforehand. After that, using Printrun Proterface, I connected to my printer and commanded the hot end to move using, again, the G-code sequence: &lt;code class=&quot;highlighter-rouge&quot;&gt;G1Z100, G1Z50, G1Z10, G1Z1, G1Z0.5&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There is a &lt;strong&gt;big&lt;/strong&gt; caveat with my Blu-Tack method: you must redo it after every print :grimacing:. However, you can use it to print a &lt;a href=&quot;http://www.thingiverse.com/thing:1732494&quot;&gt;bed leveler&lt;/a&gt; like I did.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/first_attempt_bed_leveling.jpg?style=centerme&quot; alt=&quot;Bed leveling with 3D printed parts&quot; /&gt;
  &lt;figcaption&gt;Bed leveling with 3D printed parts (Mr Focus was on holidays...).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Despite that, my printer was still not properly leveled because the range of possible heights was too small. So, I decided to design and print &lt;a href=&quot;http://www.thingiverse.com/thing:2252569&quot;&gt;my very own bed leveler&lt;/a&gt; and, while it is not perfect, finally I can use my printer without incessant recalibrations.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/my_bed_leveler.jpg?style=centerme&quot; alt=&quot;My customized bed leveler&quot; /&gt;
  &lt;figcaption&gt;My very own 3D printer bed leveler&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And everything worked fine at the end because I was able to calibrate the 3D printer using my current best-loved hacking accessory: Blu-Tack :kissing_heart:.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE (23/04/2017):&lt;/strong&gt;&lt;br /&gt;
I created a &lt;a href=&quot;http://localhost:4000/public/extras/0_bed_levelling.gcode&quot;&gt;G-CODE file&lt;/a&gt; to &lt;em&gt;automate&lt;/em&gt; the manual bed leveling a little bit. It is working with Marlin firmware (1.1.0 RC8) and you just need to &lt;em&gt;print&lt;/em&gt; that file (pressing the button to advance each step) to position the nozzle helping you to adjust the bed. However, I have no idea which side effects it could have in a printer different from mine, so &lt;strong&gt;use it at your own risk&lt;/strong&gt; and change it (any text editor with a &lt;em&gt;pure soul&lt;/em&gt;) according to your taste!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to reset your Windows 7, 8 or 10 password without black magic</title>
   <link href="http://localhost:4000/windows/2017/04/06/recover_windows_password/"/>
   <updated>2017-04-06T00:00:00-04:00</updated>
   <id>http://localhost:4000/windows/2017/04/06/recover_windows_password</id>
   <content type="html">&lt;p&gt;Currently, I’m a last year Robotics / Artificial Intelligence Ph.D candidate (don’t be shy, have a look at my &lt;a href=&quot;http://localhost:4000/publications/&quot;&gt;publications&lt;/a&gt;), father of a 7-yrs-old boy and I’m living abroad with my family since 2013. So, life is quite busy, a little bit stressful and it happens that, sometimes, I simply forget things. The other day, I was setting up a new Dell laptop (I’ve bought it really, really cheap from their UK outlet, free delivery and I even got an extra student discount!) that came with &lt;a href=&quot;https://en.wikipedia.org/wiki/Windows_10&quot;&gt;Windows 10&lt;/a&gt; and, as always, I created a very hard to guess password. It was so hard to guess that I forgot it after a week! And that’s how the idea for this post began.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/why_cant_sign_in.png?style=centerme&quot; alt=&quot;How to recover your windows password&quot; /&gt;
  &lt;figcaption&gt;This webpage was not helpful in my situation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;As I already said, I had this problem with a laptop running Windows 10 and, although I consider my &lt;a href=&quot;https://en.wiktionary.org/wiki/Google-fu&quot;&gt;google-fu&lt;/a&gt; &lt;em&gt;strong&lt;/em&gt;, I could only find a lot of instructions that didn’t work for me because I didn’t have the recovery disk and &lt;a href=&quot;https://answers.microsoft.com/en-us/insider/wiki/insider_wintp-insider_security/how-to-connect-to-a-microsoft-account-in-windows/c1614fe6-a9cd-4723-a9e0-7293d9cdfd4a&quot;&gt;my Windows was not connected to my Microsoft account&lt;/a&gt; yet. You know, I thought I would never forget a password :innocent:. Finally, &lt;a href=&quot;https://www.howtogeek.com/222262/how-to-reset-your-forgotten-password-in-windows-10/&quot;&gt;HTG&lt;/a&gt; seemed to have the answer, yet I was missing a small detail (I didn’t know what was the &lt;em&gt;Utility Manager&lt;/em&gt; to click on it) and that was only clarified after reading the &lt;a href=&quot;http://www.pcworld.com/article/2988539/windows/if-you-forget-your-windows-admin-password-try-this.html&quot;&gt;PC World&lt;/a&gt; hint saying to click on the easy-access icon.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/ease_access_icon.png?style=centerme&quot; alt=&quot;This is the easy-access icon&quot; /&gt;
  &lt;figcaption&gt;If you don't know it as well, this is the easy-access icon (&lt;a href=&quot;http://www.softicons.com/system-icons/windows-8-metro-invert-icons-by-dakirby309/folder-ease-of-access-icon&quot;&gt;source&lt;/a&gt;).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To avoid having to look somewhere else, I will write here the steps I took to get back to my Windows 10 (and it should work for Windows 7 and 8 too)!&lt;/p&gt;

&lt;h2 id=&quot;step-1&quot;&gt;Step 1&lt;/h2&gt;
&lt;p&gt;First, you will need to create a bootable Windows installer DVD / USB stick. If you are lucky enough to have a Windows DVD, since most computers, nowadays, come simply with a pre-installed version instead of a physical media, and even luckier to have a DVD drive, you can just boot from your DVD, so I suggest you to jump straight into &lt;strong&gt;Step 2&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;However, most of us don’t have the media or a DVD drive. Microsoft seems to know that and they provide a special webpage called &lt;a href=&quot;https://www.microsoft.com/en-gb/download/windows-usb-dvd-download-tool&quot;&gt;Windows USB/DVD Download Tool&lt;/a&gt; where you can download it. Actually, the &lt;a href=&quot;https://www.theregister.co.uk/2016/05/06/thoughts_on_the_new_microsoft/&quot;&gt;&lt;em&gt;new&lt;/em&gt; Microsoft&lt;/a&gt; also provides a &lt;a href=&quot;https://www.microsoft.com/en-gb/software-download/windows10ISO&quot;&gt;special page&lt;/a&gt; for people running something else but Windows.&lt;/p&gt;

&lt;p&gt;If your computer has a DVD drive, but you don’t have the Windows install DVD, you can download the ISO from Microsoft and find a place to record a DVD. You will need a second computer with a DVD recorder, anyway.&lt;/p&gt;

&lt;p&gt;The USB stick option should work for almost everybody, but it can be a little bit tricky if you don’t have a second computer with Windows installed (if you have, the &lt;a href=&quot;https://www.microsoft.com/en-gb/download/windows-usb-dvd-download-tool&quot;&gt;Windows USB/DVD Download Tool&lt;/a&gt; will do the job for you). Even if you have a spare Windows system running from virtual machine like me (Parallels 12 in my case), the program available from the Windows USB/DVD Download Tool (MediaCreationTool.exe) may not find your USB drive (that was what happened to me, even though Parallels could access and see the it). In this situation, you will need to use the ISO file, instead.&lt;/p&gt;

&lt;p&gt;Since I was using a Macbook Pro running &lt;a href=&quot;https://en.wikipedia.org/wiki/OS_X_Yosemite&quot;&gt;OSX Yosemite&lt;/a&gt;, I just launched the &lt;a href=&quot;https://support.apple.com/en-gb/boot-camp&quot;&gt;Boot Camp Assistant&lt;/a&gt; and asked it to create a Windows install disk (in fact, it was a USB stick) using a previously generated ISO file. It worked like a charm :relieved:.&lt;/p&gt;

&lt;p&gt;If you are using Linux, one of these links (&lt;a href=&quot;http://askubuntu.com/questions/289559/how-can-i-create-a-windows-bootable-usb-stick-using-ubuntu&quot;&gt;link1&lt;/a&gt; and &lt;a href=&quot;https://thornelabs.net/2013/06/10/create-a-bootable-windows-7-usb-drive-in-linux.html&quot;&gt;link2&lt;/a&gt;) may work for you.&lt;/p&gt;

&lt;h2 id=&quot;step-2&quot;&gt;Step 2&lt;/h2&gt;
&lt;p&gt;Ok, with your Windows Installer DVD or USB stick ready to rock, you need to find a way to &lt;a href=&quot;https://en.wikipedia.org/wiki/Booting&quot;&gt;boot&lt;/a&gt; your computer from it. Normally, you just need to connect it, switch on the computer and it will find the DVD/USB stick and boot from it. However, &lt;a href=&quot;https://en.wikipedia.org/wiki/Murphy%27s_law&quot;&gt;Murphy’s law&lt;/a&gt; is a powerful one and if your computer is not configured to first try to boot from the DVD/USB, you may have problems. The solution is to find out how to access the BIOS configurations to change the boot order or find a special key that lets your select the boot order. The laptop I was trying to recover was a Dell, so I just had to push the F12 key to select the boot from a USB stick.&lt;/p&gt;

&lt;p&gt;After booting from the Windows Installer DVD/USB, you will need a command prompt. One problem with the &lt;a href=&quot;https://www.howtogeek.com/222262/how-to-reset-your-forgotten-password-in-windows-10/&quot;&gt;HTG instructions&lt;/a&gt; was they say you should press &lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT+F10&lt;/code&gt;, but it was not working for me. It took me a while to find a solution because on my laptop you need to press the FN key to activate the &lt;em&gt;F-keys&lt;/em&gt; making it &lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT+FN+F10&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With the command prompt available, the &lt;em&gt;magic&lt;/em&gt; is done by altering the &lt;code class=&quot;highlighter-rouge&quot;&gt;Utilman.exe&lt;/code&gt; file. First, we need to find where Windows is installed. Try a simple &lt;code class=&quot;highlighter-rouge&quot;&gt;dir C:\Windows\System32&lt;/code&gt; and, if it worked, go ahead and make a backup of &lt;code class=&quot;highlighter-rouge&quot;&gt;Utilman.exe&lt;/code&gt; using:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;move C:\Windows\System32\Utilman.exe C:\Windows\System32\Utilman.exe.old
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After that, we want to trick Windows to launch &lt;code class=&quot;highlighter-rouge&quot;&gt;cmd.exe&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;Utilman.exe&lt;/code&gt;. This is accomplished by:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy C:\Windows\System32\cmd.exe C:\Windows\System32\Utilman.exe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, you can just type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wpeutil reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remove your USB stick after the screen becomes black and wait for Windows to boot again.&lt;/p&gt;

&lt;p&gt;Since we replaced &lt;code class=&quot;highlighter-rouge&quot;&gt;Utilman.exe&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;cmd.exe&lt;/code&gt;, you need to click on the easy-access icon and it will launch a &lt;a href=&quot;https://www.lifewire.com/command-prompt-2625840&quot;&gt;command prompt&lt;/a&gt; now (I learned this from &lt;a href=&quot;http://www.pcworld.com/article/2988539/windows/if-you-forget-your-windows-admin-password-try-this.html&quot;&gt;PC World&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;After all that, with your command prompt available, type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net user
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;find your user name and type (pay attention, you must add a space and the asterisk at the end just after the user name):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net user YOUR_USER_NAME_HERE *
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Change your password and that’s it! At least it worked for me :bowtie:.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attention:&lt;/strong&gt; don’t forget to &lt;em&gt;UNDO&lt;/em&gt; what you have done to &lt;code class=&quot;highlighter-rouge&quot;&gt;Utilman.exe&lt;/code&gt; by copying the backup version back to its old name:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;move C:\Windows\System32\Utilman.exe.old C:\Windows\System32\Utilman.exe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I hope these instruction will be useful to someone else, cheers!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 2</title>
   <link href="http://localhost:4000/deep_learning/2017/03/05/easy-peasy_conv_deep_learning_two/"/>
   <updated>2017-03-05T00:00:00-05:00</updated>
   <id>http://localhost:4000/deep_learning/2017/03/05/easy-peasy_conv_deep_learning_two</id>
   <content type="html">&lt;p&gt;This is the continuation (finally!), or the &lt;em&gt;Part 2&lt;/em&gt;, of the &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;“Easy-peasy Deep Learning and Convolutional Networks with Keras”&lt;/a&gt;. This post should be something self-contained, but you may enjoy reading &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;&lt;em&gt;Part 1&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/&quot;&gt;&lt;em&gt;Part 1½&lt;/em&gt;&lt;/a&gt;… it’s up to you.&lt;/p&gt;

&lt;p&gt;Around one week ago, I’d attended a &lt;a href=&quot;https://sites.google.com/site/cudatraining/&quot;&gt;CUDA workshop&lt;/a&gt; presented (or should I say &lt;a href=&quot;https://books.google.com/ngrams/graph?content=workshop+presented%2Cworkshop+conducted&amp;amp;year_start=1800&amp;amp;year_end=2000&amp;amp;corpus=15&amp;amp;smoothing=3&amp;amp;share=&amp;amp;direct_url=t1%3B%2Cworkshop%20presented%3B%2Cc0%3B.t1%3B%2Cworkshop%20conducted%3B%2Cc0&quot;&gt;conducted&lt;/a&gt;?) by my friend &lt;a href=&quot;https://www.facebook.com/AmityFarm/&quot;&gt;Anthony Morse&lt;/a&gt; and I’m still astounded by &lt;a href=&quot;https://developer.nvidia.com/digits&quot;&gt;DIGITS&lt;/a&gt;. So, during the workshop, I had some interesting ideas to use on this post!&lt;/p&gt;

&lt;p&gt;The first thing I thought when I read (or heard?) for the first time the name &lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; was &lt;em&gt;a bunch of &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_(image_processing)&quot;&gt;filters&lt;/a&gt;&lt;/em&gt; (&lt;a href=&quot;https://docs.gimp.org/en/plug-in-convmatrix.html&quot;&gt;Gimp would agree with me&lt;/a&gt;). I’m an Electrical Engineer and, for most of us (Electrical Engineers), convolutions start as nightmares and, gradually, become our almighty super weapon after a module like &lt;a href=&quot;https://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/&quot;&gt;Signal and Systems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s start with something easy… a &lt;a href=&quot;https://www.youtube.com/watch?v=puxHUGpuOVw&quot;&gt;video&lt;/a&gt;! Below, you can observe, step-by-step, what happens when a 2D convolution (think about a &lt;a href=&quot;https://en.wikipedia.org/wiki/Edge_detection&quot;&gt;filter&lt;/a&gt; that detects, or enhances, edges) is applied to an image:&lt;/p&gt;

&lt;div class=&quot;video-container&quot; align=&quot;center&quot;&gt;
&lt;iframe width=&quot;854&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/puxHUGpuOVw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;The &lt;em&gt;red’ish&lt;/em&gt; 3x3 square moving around the cat’s face is where the kernel (an &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_(image_processing)#Details&quot;&gt;edged detector&lt;/a&gt;) is instantaneously applied. If the terms I’m using here are giving you &lt;a href=&quot;http://dictionary.cambridge.org/dictionary/english/goosebumps&quot;&gt;goosebumps&lt;/a&gt;, try to read this first &lt;a href=&quot;http://www.wildml.com/deep-learning-glossary/&quot;&gt;deep learning glossary&lt;/a&gt; first. When I say &lt;em&gt;applied&lt;/em&gt; I mean an &lt;a href=&quot;http://www.scipy-lectures.org/intro/numpy/operations.html#id2&quot;&gt;elementwise&lt;/a&gt; multiplication where the result is presented at the bottom (small square, really pixelated or 3x3 if you prefer). The picture, on the right hand side, is the sum of the values (you can visualize them on the small square figure at the bottom) at that instant (but the scales now are different, final result uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Absolute_value&quot;&gt;absolute values&lt;/a&gt;). If you, like me, thinks my explanation above is very poor and you want to understand what is really happening (e.g. Why does the original image have a black border?), I would suggest you to have a look on these websites: &lt;a href=&quot;http://cs231n.github.io/convolutional-networks&quot;&gt;CS231n&lt;/a&gt;, &lt;a href=&quot;http://colah.github.io/posts/2014-07-Understanding-Convolutions/&quot;&gt;Colah’s&lt;/a&gt; and &lt;a href=&quot;http://intellabs.github.io/RiverTrail/tutorial/&quot;&gt;Intel Labs’ River Trail project&lt;/a&gt;. The figure below, taken from Intel’s website, is (&lt;a href=&quot;https://en.wiktionary.org/wiki/IMHO&quot;&gt;IMHO&lt;/a&gt;) &lt;em&gt;the killer&lt;/em&gt; explanation:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/convolution_intellabs.png?style=centerme&quot; alt=&quot;What happens when we do a 2D convolution&quot; /&gt;
  &lt;figcaption&gt;This is exactly what I did to create the video. &lt;i&gt;Image from &lt;a href=&quot;http://intellabs.github.io/RiverTrail/tutorial/&quot;&gt;Intel Labs' River Trail project&lt;/a&gt;&lt;/i&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Nevertheless, this post is supposed to be &lt;em&gt;easy-peasy®&lt;/em&gt; style and I will not change it now. My definition for &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolution&quot;&gt;convolution&lt;/a&gt; is: a close relative of &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt; (pay attention to &lt;em&gt;τ&lt;/em&gt; and &lt;em&gt;t&lt;/em&gt; if you decide to follow the previous link and have a closer look at the initial animation I’ve presented) and very good friend of &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot_product&quot;&gt;dot product&lt;/a&gt; (try to &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html&quot;&gt;flatten&lt;/a&gt; things and it will be easier to spot it) - even the symbols have a strong resemblance. &lt;em&gt;Improving&lt;/em&gt; even further my &lt;em&gt;easy-peasy®&lt;/em&gt; description, another way to describe what a convolution is would be by saying it is like rubbing one function (our &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_(image_processing)&quot;&gt;kernel&lt;/a&gt;) against another one and taking note of the result while you do it :satisfied:.&lt;/p&gt;

&lt;p&gt;I’m starting to hate this idea of having this &lt;em&gt;outline thing&lt;/em&gt;… but I will try to keep using it! If I got it right, after reading (writing, in my situation) everything and trying to figure out where all things came from and why, we should be able to:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Undoubtedly convince ourselves &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; is cool!&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Create our first very own &lt;strong&gt;Convolutional&lt;/strong&gt; Neural Network.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Understand there’s no magic! (but if we just had &lt;a href=&quot;https://en.wikipedia.org/wiki/Infinite_monkey_theorem&quot;&gt;enough monkeys&lt;/a&gt;…)&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Get a job on Facebook… ok, we will need a lot more to impress &lt;a href=&quot;http://yann.lecun.com/&quot;&gt;Monsieur Lecun&lt;/a&gt;, but this is a starting point :bowtie:.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;And, finally, enjoy our time while doing all the above things!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, ok, ok… let’s get back to &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; and our &lt;a href=&quot;https://www.youtube.com/watch?v=Iz-8CSa9xj8&quot;&gt;&lt;em&gt;precious&lt;/em&gt;&lt;/a&gt; Convolutional Neural Networks (&lt;a href=&quot;https://en.wikipedia.org/wiki/Aka&quot;&gt;aka&lt;/a&gt; &lt;strong&gt;CNNs&lt;/strong&gt;) :wink:.&lt;/p&gt;

&lt;p&gt;Do you remember I said CNNs were just &lt;em&gt;a bunch of filters&lt;/em&gt;? I was not lying or making fun of you. CNNs are a mixture between a fancy &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolution&quot;&gt;filter bank&lt;/a&gt; and &lt;a href=&quot;https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html&quot;&gt;Feedforward Neural Networks&lt;/a&gt;, but everything works together with the help of our friend &lt;a href=&quot;http://cs231n.github.io/optimization-2/&quot;&gt;backpropagation&lt;/a&gt;. Recalling from &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;Part 1&lt;/a&gt;, our network was initially designed only using &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;Kera’s Dense layers&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Just creates our Keras Sequential model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# The first layer will have a uniform initialization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;768&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3072&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;uniform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The ReLU 'thing' :)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Now this layer will have output dimension of 384&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;uniform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Because we want to classify between only two classes (binary), the final output is 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;softmax&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And &lt;strong&gt;what is the problem with the classical layers?&lt;/strong&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Connectionism&quot;&gt;Theoretically&lt;/a&gt;, &lt;strong&gt;none&lt;/strong&gt;. One could, somehow, train a network with enough data to do the job. However, &lt;a href=&quot;https://en.wikipedia.org/wiki/Vanishing_gradient_problem&quot;&gt;backpropagation is not so powerful&lt;/a&gt; and it would probably demand networks &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/#overview&quot;&gt;a lot bigger&lt;/a&gt; (this is my &lt;a href=&quot;https://en.wikipedia.org/wiki/TL;DR&quot;&gt;TL;DR&lt;/a&gt; explanation, so go and use your inner &lt;a href=&quot;http://lifehacker.com/5940946/20-google-search-shortcuts-to-hone-your-google-fu&quot;&gt;&lt;em&gt;google-fu&lt;/em&gt;&lt;/a&gt; if you are not satisfied :grimacing:).&lt;/p&gt;

&lt;p&gt;CNNs are, IMHO, the engineer’s approach! They are useful because they can generate (filter out) nice features from its inputs making the life easier for &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;Dense layers&lt;/a&gt; after that. Do you remember &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning_one_and_a_half/&quot;&gt;the last post of our &lt;em&gt;easy-peasy®&lt;/em&gt; series&lt;/a&gt;? Ok, I would not expect anybody to fully remember anything, but that post was created because I realized visualization of internal layers was super important and I will bring back one important figure from there:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/output_trained.png?style=centerme&quot; alt=&quot;Layers output as images&quot; /&gt;
  &lt;figcaption&gt;This is what my trained network from Part&amp;nbsp;1&amp;frac12; outputs (not the CNN!), but viewed as RGB images.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The first image is &lt;em&gt;clearly&lt;/em&gt; a dog, but the second image (composed by the output from the first layer) is totally crazy and doesn’t look like having features to distinguish dogs from cats! So, the important bit is that I (or maybe a dense layer too) can’t even find any tiny visual clue from that image that would suggest there was a dog as input. That doesn’t mean there are no distinguishable features, but the features are hard to spot.&lt;/p&gt;

&lt;p&gt;Now let’s see what happens when we use a convolutional layer:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/conv_layer1.png?style=centerme&quot; alt=&quot;Layers output as images&quot; /&gt;
  &lt;figcaption&gt;Now, our brand new CNN outputs something different from its first layer.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Probably, the first thing I would notice here is the number of images after the first layer. Our convolutional layer created 32, different, filtered versions of its input! Kind of cheating…&lt;/p&gt;

&lt;p&gt;In order to have the above result, we need to replace, in our old model, the first &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Dense&lt;/code&gt;&lt;/a&gt; layer with a &lt;a href=&quot;https://keras.io/layers/convolutional/#convolution2d&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Convolution2D&lt;/code&gt;&lt;/a&gt; one:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;border_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'same'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_layer_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ConvLayer1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;number_of_filters=32
kernel_size=(3,3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This layer is composed by 32 filters (that is why we got 32 output images after the first layer!), each filter 3x3x3 (remember our input images are colorful &lt;a href=&quot;http://stackoverflow.com/questions/25102461/python-rgb-matrix-of-an-image&quot;&gt;RGB&lt;/a&gt; ones, therefore they have one 3x3 matrix for each color), and it will output 32 images 32x32 because &lt;code class=&quot;highlighter-rouge&quot;&gt;border_mode='same'&lt;/code&gt;. It’s crystal clear that this behaviour would lead to an &lt;a href=&quot;https://en.wikipedia.org/wiki/Geometric_series&quot;&gt;explosion&lt;/a&gt; because the next layer would always be 32x bigger than the previous… but &lt;strong&gt;not&lt;/strong&gt;! If we read the &lt;a href=&quot;https://keras.io/layers/convolutional/#convolution2d&quot;&gt;Convolution2D manual&lt;/a&gt;, this layer receives a 4D &lt;a href=&quot;http://www.physlink.com/education/askexperts/ae168.cfm&quot;&gt;Tensor&lt;/a&gt;, returns a 4D Tensor (no extra dimension created) and, consequently, no explosion at all because the generated weights do the magic of combining things for us :sweat_smile:. If we verify the shape of the weights tensor, the first convolutional layer weights tensor has shape (32, 3, 3, 3) and the second convolutional layer has shape (8, 32, 3, 3). Remember, the input image is (3, 32, 32), the first layer has 32 filters and the second only 8.&lt;/p&gt;

&lt;p&gt;Even though the explosion problem does not afflict us, I will condense the features before the last fully-connected Keras &lt;code class=&quot;highlighter-rouge&quot;&gt;Dense&lt;/code&gt; layer to speed up things by reducing the total number of weights and, therefore, calculations. The solution for this is the use of a shrinking layer and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer&quot;&gt;pooling layer&lt;/a&gt; is what we need. From Keras, we will use a &lt;a href=&quot;https://keras.io/layers/pooling/#maxpooling2d&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Maxpooling2d&lt;/code&gt;&lt;/a&gt; layer with &lt;code class=&quot;highlighter-rouge&quot;&gt;pool_size=(2,2)&lt;/code&gt; and it will shrink our 32x32x32 output down to 32x16x16. The reduction from 32x32 to 16x16 is done by keeping the biggest value (probably that’s the reason of starting with “Max”) inside the 2x2 pool.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;border_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'same'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_layer_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ConvLayer2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ConvLayer2Activation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;dim_ordering&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'th'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PoolingLayer2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since this &lt;em&gt;easy-peasy®&lt;/em&gt; series was not designed to beat benchmarks, but mainly to learn and understand what is happening, I will add one more set of convolutional and pooling layers. Moreover, I will reduce the number of filters from 32 to 8. I really want to see changes on the internal weights, even if this generates overfitting, thus I’ve increased the epochs to 1000. Apart from the things I’ve explained here so far, everything else will be kept the same as the &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;Part 1&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;Part 1½&lt;/a&gt; posts.&lt;/p&gt;

&lt;p&gt;Testing our new CNN against the test set (the 25% randomly chosen images from the directory train) returned us a accuracy of… 97%! You can find the saved model &lt;a href=&quot;https://github.com/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/my_convnet_SDG.h5?raw=true&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_vs_cat_conv_test_set.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Testing the Convolutional Neural Network against images from the test set.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And below you can see what appears inside the CNN with the dog:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/conv_layer1.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;First Layer internal results using the dog image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/conv_layer2.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Second Layer internal results using the dog image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In order to help us verify the differences between dogs and cats, here are the images where a cat was the input:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/conv_layer1_cat.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;First Layer internal results using the cat image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/conv_layer2_cat.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Second Layer internal results using the cat image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After all that, I was not 100% sure if Keras was really doing convolution or cross-correlation. Instead of searching for an answer, I decided to verify it myself. First, I save the trained weights:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ConvLayer1'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;weights_conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ConvLayer2'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;weights_conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then I manually do the convolution (same previous dog picture) using Scipy &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;convolve2d&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convolve2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;weights_conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'same'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;boundary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fill'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_scipy_conv2d.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Result of Scipy convolve2d.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Followed by the cross-correlation using Scipy &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate2d.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;correlate2d&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correlate2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;weights_conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'same'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_scipy_corr2d.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Result of Scipy correlate2d.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In the end I do a convolution using a cross-correlation (remember the &lt;em&gt;τ&lt;/em&gt; and &lt;em&gt;t&lt;/em&gt; signal change):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correlate2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights_conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'same'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_scipy_corr2d_flipped.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Result of Scipy correlate2d, but flipping things (look again the convolution2d result).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;That’s it, all done!&lt;/p&gt;

&lt;p&gt;Lastly, let’s see what we have achieved:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Undoubtedly convince ourselves &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; is cool!&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Create our first very own &lt;strong&gt;Convolutional&lt;/strong&gt; Neural Network.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Understand there’s no magic! (but if we just had &lt;a href=&quot;https://en.wikipedia.org/wiki/Infinite_monkey_theorem&quot;&gt;enough monkeys&lt;/a&gt;…)&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Get a job on Facebook… ok, we will need a lot more to impress &lt;a href=&quot;http://yann.lecun.com/&quot;&gt;Monsieur Lecun&lt;/a&gt;, but this is a starting point :bowtie:.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;And, finally, enjoy our time while doing all the above things!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was almost forgetting, we can tick this box from the first &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;post&lt;/a&gt;:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Show off by modifying the previous example using a convolutional layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alright, we still need to do a lot more to get that Facebook job position… but in &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaucho&quot;&gt;Southern Brazil&lt;/a&gt;, we have a saying &lt;em&gt;“Não tá morto quem peleia”&lt;/em&gt; that I would translate as &lt;em&gt;“If you can still fight, the battle is not over”&lt;/em&gt; :smile:.&lt;/p&gt;

&lt;p&gt;As promised, &lt;a href=&quot;http://nbviewer.jupyter.org/github/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/Keras%20Cats%20and%20Dogs%20-%20convolutional%20deep%20net%20(not%20so%20deep)%20-%20Final.ipynb&quot;&gt;here&lt;/a&gt; you can visualize (or download) a &lt;a href=&quot;https://ipython.org/notebook.html&quot;&gt;Jupyter (IPython) notebook&lt;/a&gt; with all the source code and something else.&lt;/p&gt;

&lt;p&gt;And that’s all folks! I hope you enjoyed our short Keras adventure. Cheers!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to keep things running after you close your terminal (Linux/Unix)</title>
   <link href="http://localhost:4000/linux/2017/02/28/Nohup_magic/"/>
   <updated>2017-02-28T00:00:00-05:00</updated>
   <id>http://localhost:4000/linux/2017/02/28/Nohup_magic</id>
   <content type="html">&lt;p&gt;When you start playing with cloud computing like &lt;a href=&quot;https://aws.amazon.com/&quot;&gt;Amazon Web Services&lt;/a&gt;, you will, sometimes, decide to launch a program that will take a while to run. If you simply close the connection before all processes are finished, the system will terminate &lt;a href=&quot;https://www.gnu.org/software/bash/&quot;&gt;bash&lt;/a&gt; (or whatever &lt;a href=&quot;http://www.freeos.com/guides/lsst/ch01sec07.html&quot;&gt;shell&lt;/a&gt; you were using) and, therefore, your program will be also terminated. Normally, when we are working on a terminal, we make use of the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; (or &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+z&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;bg&lt;/code&gt;) to send things to the background freeing the terminal. If you sent your process to background, you will be able to use &lt;a href=&quot;https://www.cyberciti.biz/faq/unix-linux-jobs-command-examples-usage-syntax/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jobs&lt;/code&gt;&lt;/a&gt; to display information about processes that are sleeping (the &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+z&lt;/code&gt; thing) or running on the background.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;The problem here is that your processes running on the background will be terminated anyway. There are at least two solutions. The simplest one is installing &lt;a href=&quot;https://en.wikipedia.org/wiki/Nohup&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nohup&lt;/code&gt;&lt;/a&gt; on your computer (in case it’s not already available):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install nohup
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code class=&quot;highlighter-rouge&quot;&gt;nohup&lt;/code&gt;, you really just need to add it in front of your normal command and send it to the background (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;$ nohup ls &amp;amp;&lt;/code&gt;). By default, a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;nohup.out&lt;/code&gt; will be created with all the output (&lt;a href=&quot;http://stackoverflow.com/a/3385261&quot;&gt;stdout/stderr&lt;/a&gt;) generated by your program. You can find more “nohup stuff” &lt;a href=&quot;http://linux.101hacks.com/unix/nohup-command/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The second option is to manually do what &lt;code class=&quot;highlighter-rouge&quot;&gt;nohup&lt;/code&gt; does, but using &lt;code class=&quot;highlighter-rouge&quot;&gt;disown&lt;/code&gt;. This is explained &lt;a href=&quot;http://stackoverflow.com/a/625436&quot;&gt;here&lt;/a&gt;. You will need to send your process to background (&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+z&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;bg&lt;/code&gt;), verify its job number with &lt;code class=&quot;highlighter-rouge&quot;&gt;jobs -l&lt;/code&gt; and then:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ disown -h job_number_you_just_found
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;UPDATE (05/03/2017):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Today, I was launching some simulations on a server and I decided to do it using &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&amp;amp;&lt;/code&gt; to make sure the next step would only be executed if the previous had finished without errors. The &lt;a href=&quot;http://unix.stackexchange.com/questions/47230/how-to-execute-multiple-command-using-nohup&quot;&gt;solution&lt;/a&gt;, using &lt;code class=&quot;highlighter-rouge&quot;&gt;nohup&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ nohup sh -c 'first_command &amp;amp;&amp;amp; second_command' &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s it. Have a happy cloud computing experience!&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Mounting an external USB device formatted with exFAT on Linux Ubuntu</title>
   <link href="http://localhost:4000/linux/2017/02/28/Mounting_exfat_on_linux_Ubuntu/"/>
   <updated>2017-02-28T00:00:00-05:00</updated>
   <id>http://localhost:4000/linux/2017/02/28/Mounting_exfat_on_linux_Ubuntu</id>
   <content type="html">&lt;p&gt;This post is a personal reminder. I’m always forgetting Ubuntu (up to 16.04), doesn’t know how to mount &lt;a href=&quot;https://en.wikipedia.org/wiki/ExFAT&quot;&gt;exFAT - Extended File Allocation Table&lt;/a&gt; and then I need to &lt;a href=&quot;http://unixnme.blogspot.co.uk/2016/04/how-to-mount-exfat-partition-in-ubuntu.html&quot;&gt;Google for it&lt;/a&gt;. Why would you need exFAT? Among other things, it’s possible to have files bigger than 4GB.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Ok, by default Ubuntu doesn’t know how to deal with exFAT and we need to install the &lt;a href=&quot;https://en.wikipedia.org/wiki/Filesystem_in_Userspace&quot;&gt;FUSE&lt;/a&gt; module &lt;a href=&quot;https://github.com/relan/exfat&quot;&gt;exfat-fuse&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install exfat-fuse exfat-utils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s it. Ubuntu will mount it automatically for you, somewhere inside &lt;code class=&quot;highlighter-rouge&quot;&gt;/media&lt;/code&gt; and using &lt;code class=&quot;highlighter-rouge&quot;&gt;your_user_name/your_media_name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Easy-peasy Deep Learning and Convolutional Networks with Keras - Part&nbsp;1&frac12;</title>
   <link href="http://localhost:4000/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/"/>
   <updated>2017-02-12T00:00:00-05:00</updated>
   <id>http://localhost:4000/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half</id>
   <content type="html">&lt;p&gt;This is the continuation, &lt;em&gt;Part 1½&lt;/em&gt;, of the &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;“Easy-peasy Deep Learning and Convolutional Networks with Keras”&lt;/a&gt;. Do you really need to read &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;&lt;em&gt;Part 1&lt;/em&gt;&lt;/a&gt; to understand what is going on here? Honestly, probably not, but I would suggest you doing so anyway.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/output_trained.png?style=centerme&quot; alt=&quot;Layers output as images&quot; /&gt;
  &lt;figcaption&gt;This is what my trained network outputs, but viewed as RGB images.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;Before we start, let me just tell you a little bit about &lt;a href=&quot;/about/#me&quot;&gt;myself&lt;/a&gt;, as if you care…:stuck_out_tongue_winking_eye:. Am I writing these posts to give something back to the community? Yes, this is the &lt;strong&gt;main&lt;/strong&gt; reason, but I’m doing this as well because when you try to explain, or teach, something, you actually learn a lot more than just passively studying and, I would dare say, it’s even more efficient than when you are &lt;em&gt;just&lt;/em&gt; applying your knowledge to solve a real problem. So, this is not simply an &lt;a href=&quot;https://en.wikipedia.org/wiki/Altruism&quot;&gt;altruistic&lt;/a&gt; thing. However, if everybody starts behaving like this, it will naturally help all the &lt;em&gt;players&lt;/em&gt;, therefore it’s a very very nice win-win situation in my humble opinion:bowtie:.&lt;/p&gt;

&lt;p&gt;When I started writing the &lt;em&gt;Part 2&lt;/em&gt; post, I thought I knew &lt;em&gt;a lot&lt;/em&gt; about deep learning convolutional networks, although, &lt;a href=&quot;http://www.wordreference.com/fren/lentement&quot;&gt;&lt;em&gt;lentement&lt;/em&gt;&lt;/a&gt;, I realized there were lots and lots of things I need to understand better. That was the reason why, actually, I decided to go back to the &lt;a href=&quot;http://ricardodeazambuja.com/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;initial model&lt;/a&gt; and write a &lt;em&gt;Part 1½&lt;/em&gt; post. To understand why convolutional layers are cool, we need to see what is happening after each layer and this can be done with Keras &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;Dense&lt;/a&gt; layers too.&lt;/p&gt;

&lt;p&gt;I’m trying to keep using this &lt;em&gt;outcomes methodology&lt;/em&gt;, so here it comes. By the end of this post…&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Learn how images are flattened and transformed back to an image.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Visualize what happens when we don’t put together our images in a proper way.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Access internal layer outputs using Keras.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Visualize the internal layer outputs as if they where images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://ricardodeazambuja.com/deep_learning/2017/01/29/easy-peasy_deep_learning/&quot;&gt;The first post&lt;/a&gt; was based on a &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;Feedforward Neural Network&lt;/a&gt; and the images where rescaled (downsampling) and flattened (transformed into 1D arrays) using this bit of Python code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;misc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imagePath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;input_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;misc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imresize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sometimes, or most of the time, I need to think, and think again, to make sure I’m doing the right thing after I use &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.flatten&lt;/code&gt; and I need to reconstruct the original array afterwards. The matrices bellow represent a simple 2x2 RBG image (I’m using &lt;a href=&quot;http://docs.sympy.org/latest/tutorial/printing.html&quot;&gt;Sympy&lt;/a&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Matrix&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;init_printing(use_latex='png')&lt;/code&gt; to automatically transform from &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.ndarray&lt;/code&gt; to &lt;a href=&quot;http://www.latex-project.org/about/&quot;&gt;LaTeX&lt;/a&gt;):&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/simple_rgb_image_matrices.png?style=centerme&quot; alt=&quot;Matrices for a 2x2 RGB image&quot; /&gt;
  &lt;figcaption&gt;Matrices defining a 2x2 RGB image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If you use &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib.pyplot.imshow&lt;/code&gt;, the result is the following image:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/simple_rgb_image.png?style=centerme&quot; alt=&quot;2x2 RGB image&quot; /&gt;
  &lt;figcaption&gt;The 2x2 RGB image defined by the matrices above. A zero value for R,G and B means black.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;When we apply &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.flatten&lt;/code&gt; to our initial image, the &lt;code class=&quot;highlighter-rouge&quot;&gt;flatten&lt;/code&gt; method gets element (0,0) from all 3 matrices, then element (0,1)… and, at the end, just glue them together as a big 1D array.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/simple_rgb_image_matrices_flatten.png?style=centerme&quot; alt=&quot;Effects of flatten&quot; /&gt;
  &lt;figcaption&gt;From left to right: initial matrices, secondary one using the first element of each original matrix and the resultant 1D array when the columns are glued one after another.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And what does happen if you fool around with your matrices without any visual feedback? Here is a test that shows some crazy possibilities:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;141&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;142&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Crazy 1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;143&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Crazy 2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;144&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Crazy 3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code generates the images below:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/reshaped_images.png?style=centerme&quot; alt=&quot;playing with reshape&quot; /&gt;
  &lt;figcaption&gt;Testing what happens if you simply apply reshape to a flattened image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Now, just for fun, I’m also publishing the original image, but as three matrices (I know it’s impossible to read, but it’s nice to show how even a 32x32 low-resolution image can be overwhelming when seen as matrices):&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/matrices_original_image.png?style=centerme&quot; alt=&quot;original image, RGB matrices&quot; /&gt;
  &lt;figcaption&gt;This was the output of Matrix after applying numpy.round to the original image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Ok, ok, I think we already had enough :unamused: all those image reshaping, etc. All the discussion started to help us visualize what is happening inside our beloved neural networks. Keras has, at least, two different ways to access what is happening inside our model: &lt;a href=&quot;https://keras.io/getting-started/functional-api-guide/&quot;&gt;functional API&lt;/a&gt; and &lt;a href=&quot;https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer&quot;&gt;creating a new model&lt;/a&gt;. Since, by this time, this post is sufficiently long, I will skip the functional API and stick to the new model one.&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&quot;https://keras.io/getting-started/sequential-model-guide/&quot;&gt;Keras Sequential model&lt;/a&gt;, we can access each layer this way:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# loads our old model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_97perc_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'my_97perc_acc.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# creates helper variable to directly access layer instances&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;first_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_97perc_acc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;first_layer_after_activation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_97perc_acc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_97perc_acc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_layer_after_activation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_97perc_acc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code above loads our old model to &lt;code class=&quot;highlighter-rouge&quot;&gt;my_97perc_acc&lt;/code&gt;. After that, all the layer instances are available in a list (&lt;code class=&quot;highlighter-rouge&quot;&gt;my_97perc_acc.layers&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;I want to compare our trained model to a randomly initialized one, therefore I will create another Sequential model, but this time I will give names to my layers using the argument &lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# https://keras.io/getting-started/sequential-model-guide/&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# define the architecture of the network&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# input layer has size &quot;input_dim&quot; (new_image_size[0]*new_image_size[1]*3).&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The first hidden layer will have size 768, followed by 384 and 2.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 3072=&amp;gt;768=&amp;gt;384=&amp;gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# A Dense layer is a fully connected NN layer (feedforward)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# https://keras.io/layers/core/#dense&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# init=&quot;uniform&quot; will initialize the weights / bias randomly&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;uniform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Input_layer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# https://keras.io/layers/core/#activation&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# https://keras.io/activations/&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Input_layer_act&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# Now this layer will have output dimension of 384&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;uniform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hidden_layer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hidden_layer_act&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Because we want to classify between only two classes (binary), the final output is 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;softmax&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Moreover, we can give the same layer names to the model we loaded before changing the attribute &lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;first_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Input_layer&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;first_layer_after_activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Input_layer_act&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hidden_layer&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_layer_after_activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hidden_layer_act&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using same names it is super easy to swap models and, finally, we can use Keras &lt;a href=&quot;https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Model&lt;/code&gt;&lt;/a&gt; to access internal layers:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_rnd&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# model = my_97perc_acc&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Input_layer_act'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_layer_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Hidden_layer_act'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hidden_layer_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our intermediate outputs can be generated by simply calling the &lt;code class=&quot;highlighter-rouge&quot;&gt;predict&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;first_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_layer_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_layer_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# little trick to fit into a RGB image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Have you noticed there’s no call to &lt;code class=&quot;highlighter-rouge&quot;&gt;compile&lt;/code&gt;? Apparently, &lt;a href=&quot;https://github.com/fchollet/keras/issues/3074&quot;&gt;since Keras version 1.0.3&lt;/a&gt; this is not necessary anymore if you just want to use &lt;code class=&quot;highlighter-rouge&quot;&gt;predict&lt;/code&gt; without any training.&lt;/p&gt;

&lt;p&gt;The resultant images can be visualized like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;131&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original - &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;132&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;First Layer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;133&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'none'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Second Layer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below, you can see the output for the trained network (the final output is presented in the original’s image title [P(dog),P(cat)]):&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/output_trained.png?style=centerme&quot; alt=&quot;Layers output as images&quot; /&gt;
  &lt;figcaption&gt;This is what my trained network outputs, but viewed as RGB images.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And the output for the randomly initialized one:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/output_random.png?style=centerme&quot; alt=&quot;Layers output as images&quot; /&gt;
  &lt;figcaption&gt;This is what my randomly initialized network outputs, but viewed as RGB images.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I think it’s important to recall here the fact that &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;Keras Dense layer&lt;/a&gt; is a fully connected one. Consequently, each &lt;a href=&quot;https://en.wikipedia.org/wiki/Pixel&quot;&gt;pixel&lt;/a&gt; in the image labeled &lt;em&gt;First Layer&lt;/em&gt; receives inputs from &lt;strong&gt;ALL&lt;/strong&gt; pixels in the original image (the individual pixels are multiplied by a &lt;em&gt;weight&lt;/em&gt;, summed, added a &lt;em&gt;bias&lt;/em&gt; and have to pass through the &lt;em&gt;activation function&lt;/em&gt;) such situation also applies to the &lt;em&gt;Second Layer&lt;/em&gt; in relation to the &lt;em&gt;First Layer&lt;/em&gt; and, finally, to the last layer (our classifier) at the end. The operation used here to transform two input matrices (represented by pixel values and weights) into a scalar value (the number added to the bias before passing through the activation function) has a fancy name: &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot_product&quot;&gt;&lt;em&gt;dot product&lt;/em&gt;&lt;/a&gt;. This will be useful in the future…:grimacing:.&lt;/p&gt;

&lt;p&gt;Last, but not least: can you see patterns for dogs or cats in the First or Second Layers? I can’t :satisfied:.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/testing_multiple_images_internal.png?style=centerme&quot; alt=&quot;Layers output as images&quot; /&gt;
  &lt;figcaption&gt;A few examples of what the trained network generated.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I hope, by now, we can tick the boxes below:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Learn how images are flattened and transformed back to an image.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Visualize what happens when we don’t put together our images in a proper way.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Access internal layer outputs using Keras.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Visualize the internal layer outputs as if they where images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As promised, &lt;a href=&quot;http://nbviewer.jupyter.org/github/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/Keras%20Cats%20and%20Dogs%20-%20normal%20deep%20net%20(not%20so%20deep)%20-%20visualization.ipynb&quot;&gt;here&lt;/a&gt; you can visualize (or download) a &lt;a href=&quot;https://ipython.org/notebook.html&quot;&gt;Jupyter (IPython) notebook&lt;/a&gt; with all the source code and something else :wink:.&lt;/p&gt;

&lt;p&gt;In the next post, we &lt;del&gt;will&lt;/del&gt; are going to see how to convert our simple &lt;em&gt;deep&lt;/em&gt; neural network to a convolutional neural network. Cheers!&lt;/p&gt;

&lt;!---
&lt;div class=&quot;message&quot;&gt;
  This is a draft... yep, I'm learning how to use Jekyll and I do test things on the production website :bowtie:
&lt;/div&gt;
---&gt;
</content>
 </entry>
 
 <entry>
   <title>Running a Jupyter Notebook (IPython) on a remote server</title>
   <link href="http://localhost:4000/jupyter_notebooks/2017/02/10/Jupyter_notebook_remotelly/"/>
   <updated>2017-02-10T00:00:00-05:00</updated>
   <id>http://localhost:4000/jupyter_notebooks/2017/02/10/Jupyter_notebook_remotelly</id>
   <content type="html">&lt;p&gt;Do you know what is a &lt;a href=&quot;http://jupyter.org/&quot;&gt;Jupyter Notebook&lt;/a&gt;? If you don’t, please, have a look at the previous link and come back later… just joking… ok, seriously, check the previous link because they will do a much better job explaining what is a Jupyter Notebook than me :wink:.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/jupyter_notebook.jpg?style=centerme&quot; alt=&quot;jupyter notebook&quot; /&gt;
  &lt;figcaption&gt;This is what I was doing just before start writing this post.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;Why do I love those notebooks? Probably it’s started long time ago while I still was a &lt;a href=&quot;https://www.wolfram.com/mathematica/&quot;&gt;Mathematica&lt;/a&gt;, &lt;a href=&quot;http://www.maplesoft.com/products/maple/&quot;&gt;Maple&lt;/a&gt; and &lt;a href=&quot;https://uk.mathworks.com/discovery/mupad.html&quot;&gt;MuPad&lt;/a&gt; user, but I would say I like notebooks mainly because I can keep, in one place, code, comments (pure text, &lt;a href=&quot;http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html&quot;&gt;Markdown&lt;/a&gt;, &lt;a href=&quot;https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/&quot;&gt;HTML&lt;/a&gt;, &lt;a href=&quot;http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Typesetting%20Equations.html&quot;&gt;LaTeX&lt;/a&gt;, &lt;a href=&quot;http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/&quot;&gt;animations&lt;/a&gt;, &lt;a href=&quot;http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Part%205%20-%20Rich%20Display%20System.ipynb#Video&quot;&gt;external images/videos&lt;/a&gt;, etc) and, the most important, results. I can also easily share things by exporting a notebook as HTML or PDF or uploading it to a place where there’s a notebook viewer (you can see an example &lt;a href=&quot;https://github.com/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/Keras%20Cats%20and%20Dogs%20-%20normal%20deep%20net%20(not%20so%20deep).ipynb&quot;&gt;here&lt;/a&gt;). I’ve never tested Jupyter with other languages but Python, so I will tell you about my experiences using Python. In the past, Jupyter notebooks used to be called IPython notebooks, therefore it was Python only. However, currently it supports more than 40 programming languages including &lt;a href=&quot;https://ipython.org/notebook.html&quot;&gt;Python&lt;/a&gt;, &lt;a href=&quot;https://irkernel.github.io/&quot;&gt;R&lt;/a&gt;, &lt;a href=&quot;https://github.com/JuliaLang/IJulia.jl&quot;&gt;Julia&lt;/a&gt;, &lt;a href=&quot;https://github.com/SciRuby/iruby&quot;&gt;Ruby&lt;/a&gt; and &lt;a href=&quot;https://www.scala-lang.org/&quot;&gt;Scala&lt;/a&gt; (this is the first time I’ve read about Scala…).&lt;/p&gt;

&lt;p&gt;Ok, let’s make a list of the things we want to learn here:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Launch a Jupyter Notebook server without automatically opening a browser.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Create a SSH tunnel to redirect a local port to the server.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Access your remote server from your browser.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jupyter is capable to serve, just like a &lt;a href=&quot;https://en.wikipedia.org/wiki/Web_server&quot;&gt;web server&lt;/a&gt;, notebooks directly to any browser. They give you full instructions &lt;a href=&quot;http://jupyter-notebook.readthedocs.io/en/latest/public_server.html&quot;&gt;here&lt;/a&gt;. The problem with that solution, in my opinion, is the complexity. If you just carelessly open things, you are going to expose your server :scream:.&lt;/p&gt;

&lt;p&gt;To launch the Jupyter Notebook server (on the remote computer) and keep it running after we logout, we will need help from our friend &lt;a href=&quot;https://en.wikipedia.org/wiki/Nohup&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nohup&lt;/code&gt;&lt;/a&gt;. Nohup allows us to logout without killing the processes our terminal started and the ampersand (&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt;) sends &lt;code class=&quot;highlighter-rouge&quot;&gt;nohup&lt;/code&gt; to the background &lt;em&gt;freeing&lt;/em&gt; the terminal (in case you want to do something else after launching it…). The &lt;a href=&quot;http://stackoverflow.com/a/31953548&quot;&gt;command&lt;/a&gt; to launch the notebook without also launching the web browser (even if you are accessing it remotely through &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh&lt;/code&gt; using the &lt;code class=&quot;highlighter-rouge&quot;&gt;-X&lt;/code&gt; option to redirect the display, it would be a waste of resources and time):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ nohup jupyter notebook --no-browser &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;message&quot;&gt;
  Caveat: You will only be able to access files (and subdirs) located on the current directory.
&lt;/div&gt;

&lt;p&gt;After that, you can disconnect from the remote computer, open a terminal on the client computer and type this (more details about the &lt;em&gt;&lt;a href=&quot;https://linux.die.net/man/1/ssh&quot;&gt;ssh&lt;/a&gt; tunneling magic&lt;/em&gt; can be found &lt;a href=&quot;http://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh -nNT -L 9999:localhost:8888 user@example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The command line above has only two things you surely must change: &lt;code class=&quot;highlighter-rouge&quot;&gt;user&lt;/code&gt; is the username at the remote computer and &lt;code class=&quot;highlighter-rouge&quot;&gt;example.com&lt;/code&gt; will probably be replaced by the remote computer’s IP address. Now we have created a tunnel that will redirect the &lt;a href=&quot;https://en.wikipedia.org/wiki/Port_(computer_networking)&quot;&gt;port&lt;/a&gt; 9999 to the remote server port 8888 (this is the default used by Jupyter). I’m using the port number 9999 because, usually, I also have a local Jupyter Notebook server running at 8888.&lt;/p&gt;

&lt;p&gt;The only thing you need to do now is launch your browser and access the url &lt;del&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;localhost:8888&lt;/code&gt;&lt;/del&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;localhost:9999&lt;/code&gt; and done, you have got a remote Jupyter Notebook working!&lt;/p&gt;

&lt;p&gt;Recalling the list at the beginning of this post:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Launch a Jupyter Notebook server without automatically opening a browser.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Create a SSH tunnel to redirect a local port to the server.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Access your remote server from your browser.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;UPDATE (05/03/2017):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you execute the command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh -nNT -L 9999:localhost:8888 user@example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will be capable to access the port 8888 from &lt;code class=&quot;highlighter-rouge&quot;&gt;example.com&lt;/code&gt;, on your local machine, using &lt;code class=&quot;highlighter-rouge&quot;&gt;localhost:9999&lt;/code&gt;. However, if you want to allow other machines to easily access that port too, it will not be possible. I had this problem last week when I was trying to show some contents on &lt;a href=&quot;https://www.ald.softbankrobotics.com/en/cool-robots/pepper&quot;&gt;Pepper’s&lt;/a&gt; tablet through &lt;a href=&quot;https://docs.python.org/2/library/simplehttpserver.html&quot;&gt;a very simple Python web server&lt;/a&gt; hosted on a virtual machine. My team from &lt;a href=&quot;https://github.com/ricardodeazambuja/Hackathon-Plymouth-2017&quot;&gt;NAO Hackathon&lt;/a&gt; was in a hurry and I could not make the redirection work (I should have read the &lt;a href=&quot;https://linux.die.net/man/1/ssh&quot;&gt;manual&lt;/a&gt;:disappointed:). Ok, so there are at least three possible solutions. The first one will work if your computer (the client here) has only one network address:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh -nNT -L :9999:localhost:8888 user@example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If your computer has more than one network adapter, you can &lt;em&gt;bind&lt;/em&gt; an IP (forcing the tunnel only through the bound IP) using this command (the second possible solution):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh -nNT -L ip_you_want_to_bind:9999:localhost:8888 user@example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The third possibility (&lt;em&gt;SSH-BASED VIRTUAL PRIVATE NETWORKS&lt;/em&gt;) can be found on the &lt;a href=&quot;https://linux.die.net/man/1/ssh&quot;&gt;ssh manual webpage&lt;/a&gt; (or, in case you want a more straightforward link, &lt;a href=&quot;http://superuser.com/a/311863&quot;&gt;here&lt;/a&gt;) and it involves more than one command line (the &lt;code class=&quot;highlighter-rouge&quot;&gt;-f&lt;/code&gt; argument only sends ssh to background) and you will need to mess with &lt;code class=&quot;highlighter-rouge&quot;&gt;ifconfig&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;route&lt;/code&gt;. I’m not sure if &lt;code class=&quot;highlighter-rouge&quot;&gt;route&lt;/code&gt; works the same on all Unix flavours, but apparently it &lt;a href=&quot;https://en.wikipedia.org/wiki/Route_(command)&quot;&gt;does&lt;/a&gt;. I think this last solution is a more robust one when the connection is supposed to be perpetual instead of something to run during some minutes only.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE (11/03/2017):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you redirect the port using the commands I’ve presented above, you must call &lt;code class=&quot;highlighter-rouge&quot;&gt;%matplotlib inline&lt;/code&gt; if you want to generate plots or you will also need to redirect the display, but this may not be available if the external server doesn’t have a screen (ok, you could use &lt;a href=&quot;http://askubuntu.com/a/633805&quot;&gt;this&lt;/a&gt;). &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh&lt;/code&gt; is full of useful &lt;a href=&quot;http://matt.might.net/articles/ssh-hacks/&quot;&gt;tricks&lt;/a&gt; :wink:.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1</title>
   <link href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/"/>
   <updated>2017-01-29T00:00:00-05:00</updated>
   <id>http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;Deep learning&lt;/a&gt;… wow… this is “the” &lt;a href=&quot;http://fortune.com/ai-artificial-intelligence-deep-machine-learning/&quot;&gt;hot topic&lt;/a&gt; since, at least, some good years ago! I’ve attended a few seminars and workshops about deep learning, nevertheless I’ve never tried to code something myself - until now! - because I had always another &lt;a href=&quot;http://www.tastefullyoffensive.com/2013/09/the-12-types-of-procrastnators.html&quot;&gt;priority&lt;/a&gt;. Also, I have to admit, I thought it was a lot harder and it would need much more time to be able to run anything that was not simply a sample code.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_vs_cat_test_set_start.png?style=centerme&quot; alt=&quot;Classification of Dogs and Cats&quot; /&gt;
  &lt;figcaption&gt;Example of classification results.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’m always forgetting things, so I like to take notes as if I was teaching a toddler. Consequently, this post was designed to remember myself when I forget how to use Keras :expressionless:.&lt;/p&gt;

&lt;p&gt;All the things I’ll explain below will only make sense if you know what is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;&gt;Multilayer perceptron&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;Feedforward neural network&lt;/a&gt; as well. In case you don’t, no worries, Google is your friend :stuck_out_tongue_winking_eye:.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
  Keras is a high-level neural networks library for TensorFlow and Theano. I would call it a Python wrapper that hides the extra details necessary to create neural networks... simplifying our life!
&lt;/div&gt;

&lt;p&gt;Since I’ve just learned how &lt;a href=&quot;http://blog.winddweb.info/implement-github-like-checkbox&quot;&gt;to create Github Markdown check boxes&lt;/a&gt;, let’s write down an outline of what we want to achieve at the end:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Convince ourselves learning &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; is a nice investment!&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Create our very own first deep neural network (ok, not &lt;em&gt;that&lt;/em&gt; deep) applying it to a well known task.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Show off by modifying the previous example using a convolutional layer.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Enjoy our time because when you work on something you like, it is not work anymore!&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;Am I going to reinvent the wheel? Hopefully not! I will use my very strong &lt;a href=&quot;https://en.wiktionary.org/wiki/Google-fu&quot;&gt;&lt;em&gt;Google-fu&lt;/em&gt;&lt;/a&gt; to find something we can reuse. The &lt;strong&gt;first result&lt;/strong&gt; Google gave me was &lt;a href=&quot;http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/&quot;&gt;this&lt;/a&gt;. The &lt;a href=&quot;http://www.pyimagesearch.com&quot;&gt;pyimagesearch&lt;/a&gt; website is a very good source of things related to image processing, but, I’ll have to admit, I don’t like the way the guy deals with his readers &lt;del&gt;forcing&lt;/del&gt; pushing them to use his own library and to subscribe to be able to download source code… however, he is sharing knowledge and this is a good thing :relieved:. My intention here is to partially follow his steps with some changes introduced because I thought were useful or just a matter of personal taste :satisfied: (I’m using a lot of emojis because I’ve just found this &lt;a href=&quot;http://www.webpagefx.com/tools/emoji-cheat-sheet/&quot;&gt;emoji cheat sheet&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;By the way, I’m supposing &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; and &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt; (or &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;) are already installed, up and running. My personal experience tells me Theano is easier to install than TensorFlow, but, maybe, I was just unlucky/lucky :sunglasses:.&lt;/p&gt;

&lt;p&gt;I’m using Theano on a laptop that has a GeForce GT 750M GPU, but I have found one caveat related to the amount of memory available (&lt;del&gt;the system shares its main memory with the GPU&lt;/del&gt; &lt;em&gt;I’ve found out it actually has 2GB of dedicated GDDR5 memory&lt;/em&gt;). To have more control, I’ve created a file in my user’s home directory (&lt;code class=&quot;highlighter-rouge&quot;&gt;~/&lt;/code&gt;) called &lt;em&gt;.theanorc&lt;/em&gt; with this content:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[global]
floatX=float32
device = gpu0

[lib]
cnmem = .7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Theano can run on CPU or CPU+GPU. It will autonomously choose the CPU only mode if the GPU is not available. However, my system has a GPU and I was not able to activate its use on Theano. That’s the reason why I created the &lt;em&gt;.theanorc&lt;/em&gt; file. The line &lt;code class=&quot;highlighter-rouge&quot;&gt;device = gpu0&lt;/code&gt; forces Theano to use the GPU (if you have more than one, maybe it will not be &lt;code class=&quot;highlighter-rouge&quot;&gt;gpu0&lt;/code&gt;) and the line &lt;code class=&quot;highlighter-rouge&quot;&gt;cnmem = .7&lt;/code&gt; sets the amount of memory used by &lt;a href=&quot;http://deeplearning.net/software/theano/library/config.html#config.config.lib.cnmem&quot;&gt;CNMeM&lt;/a&gt;. If you don’t have enough memory available (&lt;del&gt;laptops usually share the main memory with the GPU&lt;/del&gt;), it will give you an error message. Setting &lt;code class=&quot;highlighter-rouge&quot;&gt;cnmem = 0&lt;/code&gt; disables it.&lt;/p&gt;

&lt;p&gt;Before we start &lt;em&gt;deep learning&lt;/em&gt;, we are going to need the data set from &lt;a href=&quot;https://www.kaggle.com/c/dogs-vs-cats&quot;&gt;Kaggle Dogs vs. Cats&lt;/a&gt;. This data set has 25000 images divided into training (12500) and testing (12500) sets. The training one has the filenames like these examples: &lt;code class=&quot;highlighter-rouge&quot;&gt;cat.3141.jpg&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;dog.3141.jpg&lt;/code&gt;, while in the testing set the files are only a number with the &lt;code class=&quot;highlighter-rouge&quot;&gt;.jpg&lt;/code&gt; extension. I’m &lt;strong&gt;not&lt;/strong&gt; trying to beat a state of art algorithm (&lt;a href=&quot;http://xenon.stanford.edu/~pgolle/papers/dogcat.pdf&quot;&gt;not even an old one&lt;/a&gt;), but only to learn how to use Keras. Our network should be able to gives us an answer something like a &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; if it is a cat and &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; if it is a dog.&lt;/p&gt;

&lt;p&gt;If you have a look at the images you just downloaded, you will notice they’re not all the same size. Later, we will need resize them (our network accepts only a fixed size input) and simplify things or it will take ages to train, run, etc. I will consider the data sets are, now, installed in two directories (folders, for the young ones) named: &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;test1&lt;/code&gt;. We will need to read the images and store their filenames as well. Since we will have a loop going on, the images will pass through a downsampling to reduce their sizes too. I will use &lt;code class=&quot;highlighter-rouge&quot;&gt;scipy.misc.imresize&lt;/code&gt; because, IMHO, it’s a lot easier to install &lt;a href=&quot;https://pypi.python.org/pypi/Pillow/2.2.1&quot;&gt;PIL/Pillow&lt;/a&gt; than &lt;a href=&quot;https://www.google.co.uk/webhp?q=installing+opencv+python&quot;&gt;OpenCV&lt;/a&gt; :innocent:.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
  Summarizing: it's necessary to install Theano (or TensorFlow), Keras, Scipy (Numpy) and Pillow (if it was not automatically installed with Scipy).
&lt;/div&gt;

&lt;p&gt;Even though it’s not a good practice, I will import the packages only when they are necessary. Python is just fine with that and it is so clever that it will not import twice the same thing. At the end I will supply a file with the whole source code and, then, the imports will be all at the top (if I don’t miss some…:cold_sweat:).&lt;/p&gt;

&lt;p&gt;Another &lt;em&gt;small&lt;/em&gt; detail: the neural network we will create here only accepts one dimensional (1D) vector (or list or array, you choose the name). For that reason, we will flatten (transform it into a 1D thing) after resizing (downsampling) the images.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# The package 'os' will be necessary to find the current&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# working directory os.getcwd() and also to list all&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# files in a directory os.listdir().&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# As I've explained somewhere above, Scipy will help us&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# reading (using Pillow) and resizing images.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.misc&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Returns the current working directory&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (where the Python interpreter was called)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getcwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# The path separator used by the OS:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This is the directory where the training images are located&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;train&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Generates a list of all images (actualy the filenames) from the training set,&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# but it will also include the full path&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imagePaths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;
                  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, we should have a list (&lt;code class=&quot;highlighter-rouge&quot;&gt;imagePaths&lt;/code&gt;) with all the filenames (full path) for the images in the training set. The obvious next step is to read all those images and convert to something readable to our &lt;em&gt;deep net&lt;/em&gt;. Reading things from a hard drive is always slow and I don’t like to waste time, therefore we will use &lt;em&gt;parallel processing powers&lt;/em&gt; from Python &lt;a href=&quot;https://docs.python.org/2/library/multiprocessing.html&quot;&gt;multiprocessing&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# To speed up things, I will use parallel computing!&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Pool lets me use its map method and select the number of parallel processes.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;multiprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pool&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;import_training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
    Reads the image from imagePath, resizes
    and returns it together with its label and
    original index.

    '''&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imagePath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Reads the image&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;misc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imagePath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Split will literally split a string at that character&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# returning a list of strings.&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# First we split according to the os.path.sep and keep&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# only the last item from the list gererated ([-1]).&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# This will give us the filename.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imagePath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Then we split it again using &quot;.&quot;  &lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# and extract the first item ([0]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# and the second item ([1]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;original_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Resizes the image (downsampling) to new_image_size and&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# converts it to a 1D list (flatten).&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;misc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imresize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# The size of the resized image.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# After we apply the flatten method, it will become a list of 32x32x3 items (1024x3).&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (Where is the 'x3' coming from? Our images are composed of three colors!)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;number_of_parallel_processes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# When the map is finished, we will receive a list with tuples:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (index, 'category', 1D resized image)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# There's no guarantee about the ordering because they are running in parallel.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_parallel_processes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;import_training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                                                                  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imagePaths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Because import_training_set returns a tuple like this:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (index,(original_index,label,input_vector))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# and index is unique, we can convert to a dictionary&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to solve our problem with unordered items:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Gives a hint to the garbage collector...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Our poor neural network can only understand numbers, so we will convert our labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;cat&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;dog&lt;/code&gt;) to integers, but the Python code will be a little bit more generic. The result will be: &lt;code class=&quot;highlighter-rouge&quot;&gt;{'cat': 1, 'dog': 0}&lt;/code&gt;. Another nice thing to do is the &lt;a href=&quot;http://stackoverflow.com/a/4674770&quot;&gt;normalization of input variables&lt;/a&gt; (&lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/&quot;&gt;the source referenced on stackoverflow&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Let's imagine we don't know how many different labels we have.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (we know, they are 'cat' and 'dog'...)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# A Python set can help us to create a list of unique labels.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (i[0]=&amp;gt;filename index, i[1]=&amp;gt;label, i[2]=&amp;gt;1D vector)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;unique_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# With a list of unique labels, we will generate a dictionary&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to convert from a label (string) to a index (integer):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels2int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# Creates a list with labels&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (ordered according to the dictionary index)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels2int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ok, at the very beginning I asked about Multilayer Perceptron (MLP), Feedforward Neural Network (F… NN), etc. So, this type of NN, typically, works with &lt;a href=&quot;https://en.wikipedia.org/wiki/Floating_point&quot;&gt;real valued numbers&lt;/a&gt; essentially doing multiplications (also additions). If you enter a value zero (and there’s no &lt;a href=&quot;http://stackoverflow.com/a/2499936&quot;&gt;bias&lt;/a&gt;) it will return you zero. Why am I telling you this? I thought it was something important :satisfied:.&lt;/p&gt;

&lt;p&gt;Just to refresh your memory, I’m working based on the &lt;a href=&quot;http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/&quot;&gt;pyimagesearch post&lt;/a&gt;. This means I will use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;&gt;cross entropy&lt;/a&gt; loss function (&lt;a href=&quot;https://keras.io/metrics/#binary_crossentropy&quot;&gt;Keras binary_crossentropy&lt;/a&gt;) as well, forcing us to format the labels as vectors &lt;code class=&quot;highlighter-rouge&quot;&gt;[0.,1.]&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;[1.,0.]&lt;/code&gt; and this will be the kind of answer our network will give us at the end.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Necessary for to_categorical method.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np_utils&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Instead of '1' and '0', the function below will transform our labels&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# into vectors [0.,1.] and [1.,0.]:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np_utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_categorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you train a NN you need to have a way to test if it is learning. This is accomplished by reserving a piece of the training set for testing the network. Nevertheless, you can’t just save 75% of the images… you do it &lt;a href=&quot;http://stats.stackexchange.com/questions/248048/neural-networks-why-do-we-randomize-the-training-set/248053&quot;&gt;randomly&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# First, we will create a numpy array going from zero to len(training_set):&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (dtype=int guarantees they are integers)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random_selection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Then we create a random state object with our seed:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (the seed is useful to be able to reproduce our experiment later)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12345&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Finally we shuffle the random_selection array:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# we will use 25% for testing purposes&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Breaking the code below to make it easier to understand:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# =&amp;gt; training_set[i][-1]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Returns the 1D vector from item 'i'.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# =&amp;gt; training_set[i][-1]/255.0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Normalizes the input values from 0. to 1.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# =&amp;gt; random_selection[:int(len(training_set)*(1-test_size))]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Gives us the first (1-test_size)*100% of the shuffled items&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;trainLabels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainLabels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;testLabels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testLabels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we will create our &lt;em&gt;deep neural network&lt;/em&gt; model! I’ll not explain everything because &lt;a href=&quot;https://keras.io/getting-started/sequential-model-guide/&quot;&gt;Keras guide to Sequential model&lt;/a&gt; is already easy-peasy. This neural network will have three layers, all the layer will use Keras &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;Dense&lt;/a&gt; type. This layer type accepts a lot of arguments, but, for the first layer, we will only pass three: output dimension (output_dim), &lt;a href=&quot;https://keras.io/getting-started/sequential-model-guide/#specifying-the-input-shape&quot;&gt;input dimension (input_dim)&lt;/a&gt;, &lt;a href=&quot;https://keras.io/initializations/&quot;&gt;initialization (init)&lt;/a&gt;. After this first layer, we will add an &lt;a href=&quot;https://keras.io/activations/&quot;&gt;Activation&lt;/a&gt; as &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;rectified linear unit (ReLU)&lt;/a&gt;. The second layer will only need two arguments (output_dim and initialization), since it can do automatic shape inference. The last one will have only two inputs because we want to classify between two classes (binary) and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;&gt;Softmax&lt;/a&gt; activation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Just creates our Keras Sequential model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# The first layer will have a uniform initialization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;768&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3072&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;uniform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The ReLU 'thing' :)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Now this layer will have output dimension of 384&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;uniform&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Because we want to classify between only two classes (binary), the final output is 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;softmax&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After finishing the network creation, we need to &lt;a href=&quot;https://keras.io/getting-started/sequential-model-guide/#compilation&quot;&gt;compile&lt;/a&gt; our masterpiece. Keras has a lot of options for &lt;a href=&quot;https://keras.io/optimizers/&quot;&gt;optimizers&lt;/a&gt;, so let’s try more than one and compare. The first one will be the example from Keras guide (Root Mean Square Propagation).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# for a binary classification problem&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmsprop'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After compilation comes &lt;a href=&quot;https://keras.io/getting-started/sequential-model-guide/#training&quot;&gt;training&lt;/a&gt; (the &lt;code class=&quot;highlighter-rouge&quot;&gt;.fit&lt;/code&gt; method).  Training has even more &lt;a href=&quot;https://keras.io/models/sequential/#fit&quot;&gt;arguments&lt;/a&gt; than the compilation step! I’m not going to talk about all them, but these ones (from Keras documentation):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;batch_size:&lt;/strong&gt; integer, the number of samples per gradient update.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;nb_epoch:&lt;/strong&gt; integer, the number of epochs to train the model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verbose:&lt;/strong&gt; 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;initial_epoch:&lt;/strong&gt; epoch at which to start training (useful for resuming a previous training run).&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test score:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test accuracy:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Using the Root Mean Square Propagation, our network was basically random (accuracy 51.1520%). Let’s try another &lt;a href=&quot;https://keras.io/optimizers/&quot;&gt;optimizer&lt;/a&gt;, the first one from Keras list: &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Dradient Descent&lt;/a&gt; (SDG). If you are using IPython, don’t forget to recreate your model before you try to compile it again.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.optimizers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;binary_crossentropy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test score:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test accuracy:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Wow! Now the accuracy jumped to 77.2480%! That said, remember the &lt;a href=&quot;http://xenon.stanford.edu/~pgolle/papers/dogcat.pdf&quot;&gt;2008 paper&lt;/a&gt; already got 82.7% :disappointed_relieved:. Let’s try one last optimizer to see if something happens, this time it will be the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad&quot;&gt;Adaptive Gradient Algorithm&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.optimizers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adagrad&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adagrad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;binary_crossentropy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testLabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test score:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test accuracy:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The accuracy on the test data was 67.1840%. Probably there’s a reason why SDG is the first one on Keras documentation about optimizers. In addition, we are not fiddling with the parameters, e.g. nb_epoch. According to this &lt;a href=&quot;https://www.kaggle.com/c/dogs-vs-cats/forums/t/6845/example-use-decaf-via-nolearn-for-94-accuracy&quot;&gt;discussion&lt;/a&gt;, using DeCAF (now &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt;) via &lt;a href=&quot;https://github.com/dnouri/nolearn&quot;&gt;nolearn&lt;/a&gt; the accuracy was 94%!&lt;/p&gt;

&lt;p&gt;After all our hard work, you should save your model. Keras models have methods for saving and loading a model. Everything is nicely explained on the &lt;a href=&quot;https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model&quot;&gt;F.A.Q.&lt;/a&gt; and the simplest way is &lt;code class=&quot;highlighter-rouge&quot;&gt;model.save('my_model.h5')&lt;/code&gt; to save and &lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'my_model.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt; to load everything (the architecture , the weights, the training configuration - loss, optimizer - and the state of the optimizer).&lt;/p&gt;

&lt;p&gt;And then I decided to fiddle with the parameters using the SDG optimizer. What did I do? I simply changed &lt;code class=&quot;highlighter-rouge&quot;&gt;nb_epoch=20&lt;/code&gt; and, &lt;em&gt;voilà&lt;/em&gt;, the accuracy went up to… 97.74%! You can find the saved model &lt;a href=&quot;https://github.com/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/my_97perc_acc.h5?raw=true&quot;&gt;here&lt;/a&gt;. But, gosh, why was it so good now? In order to try to understand what happened, we must go back to where we defined our neural network. For the first two layers, we added this argument: &lt;code class=&quot;highlighter-rouge&quot;&gt;init=&quot;uniform&quot;&lt;/code&gt;. Therefore, those layers had their weights randomly assigned (&lt;a href=&quot;https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)&quot;&gt;uniform distribution&lt;/a&gt;). A further reason could be some &lt;strong&gt;crazy&lt;/strong&gt;  &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot;&gt;overfitting&lt;/a&gt; situation because the number of epochs were reduced from 50 to 20 and overfitting would usually occur when you train for too long. In the future, I will try to fight overfitting using the &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf&quot;&gt;dropout&lt;/a&gt; technique &lt;a href=&quot;https://keras.io/layers/core/#dropout&quot;&gt;already available in Keras&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here are the results from the best neural network (the 97.74% accuracy one) using images from the test set (the 25% randomly chosen images from the directory train):&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_vs_cat_test_set.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Testing the Neural Network against images from the test set.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;UPDATE (11/02/2017): I was not normalizing the images before sending them to the network!&lt;/strong&gt; I was reading again this post to start the &lt;a href=&quot;http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning_one_and_a_half/&quot;&gt;&lt;em&gt;Part 1½&lt;/em&gt;&lt;/a&gt; when I realized the outputs were always saturating (1 or 0) and then I noticed the problem with the lack of normalization:sweat_smile:.&lt;/p&gt;

&lt;p&gt;And using images never seen before (directory test1):&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/dog_vs_cat_novel.png?style=centerme&quot; alt=&quot;dogs-vs-cats&quot; /&gt;
  &lt;figcaption&gt;Testing the Neural Network against images it has never seen before.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Ok, let’s see what we have achieved so far:&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Convince ourselves learning &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; is a nice investment!&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Create our very own first deep neural network (ok, not &lt;em&gt;that&lt;/em&gt; deep) applying it to a well known task.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Show off by modifying the previous example using a convolutional layer.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Enjoy our time because when you work on something you like, it is not work anymore!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As promised, &lt;a href=&quot;http://nbviewer.jupyter.org/github/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/Keras%20Cats%20and%20Dogs%20-%20normal%20deep%20net%20(not%20so%20deep).ipynb&quot;&gt;here&lt;/a&gt; you can visualize (or download) a &lt;a href=&quot;https://ipython.org/notebook.html&quot;&gt;Jupyter (IPython) notebook&lt;/a&gt; with all the source code and something else.&lt;/p&gt;

&lt;p&gt;In the next post, we will see how to convert our simple &lt;em&gt;deep&lt;/em&gt; neural network to a convolutional neural network. Cheers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE (15/02/2017): Part 1½ is available &lt;a href=&quot;http://localhost:4000/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/&quot;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;!---
&lt;div class=&quot;message&quot;&gt;
  This is a draft... yep, I'm learning how to use Jekyll and I do test things on the production website :bowtie:
&lt;/div&gt;
---&gt;
</content>
 </entry>
 
 <entry>
   <title>My Old Projects or Why You Should Document and Share Your Stuff Online</title>
   <link href="http://localhost:4000/projects/2016/12/19/old_projects/"/>
   <updated>2016-12-19T00:00:00-05:00</updated>
   <id>http://localhost:4000/projects/2016/12/19/old_projects</id>
   <content type="html">&lt;p&gt;Today, I’ve decided to write something to make sure I’m not going to forget, again, about my old projects. Another reason for this post is to be an incentive for sharing / publishing your work online. If I had published everything online, as I was developing, I would not need to write a post about things from the past like I’m doing right now ;)  &lt;/p&gt;

&lt;p&gt;When I was still an undergrad student (2000-2005), mobile phones with good quality cameras were not available and only in 2004 (or 2005?) I got one with a really noisy, probably VGA, camera.  I was only able to have such a phone because I found an unsolvable bug on my old one, while still on guarantee, and they had no choice but give me a better one (Brazil has some really nice consumer protection laws). Also, Dropbox, GoogleDrive, etc were not available and it was quite common to lose data from time to time after a problem with a hard drive.&lt;/p&gt;

&lt;p&gt;During my years as an undergrad Electrical Engineering student, I’ve developed some interesting projects, but most of them I’ve not saved any picture, schematic, etc. For one of my analog electronics modules, my group designed a circuit to multiply two input signals using only transistors. I still remember that Analog Devices had one IC that could do exactly what we struggled to build for that module. The control systems module demanded us to develop an analog PID controller and I implemented a controller for CPU fans. Later on, I’ve worked on a Neural Network implementation using FPGA that should be able to recognise simple numbers. All those projects are lost, since I don’t have backup of anything or photos. My first project to have a digital picture saved was during the microcontrollers module. My group designed a home automation system, based on 8051, where you could activate relays through DTMF. Here is the picture (we recycled my old 56K USRobotics faxmodem plastic enclosure):&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/DARVIT_2.jpg?style=centerme&quot; alt=&quot;DARVIT&quot; /&gt;
  &lt;figcaption&gt;DARVIT - 8051 based home automation system.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;Luckily, my final project was preserved (thanks to the aforementioned noisy mobile camera and better backup systems) and I’ve managed to upload it to github too (&lt;a href=&quot;https://github.com/ricardodeazambuja/TEXvid&quot;&gt;TEXVID&lt;/a&gt;). It was based on the Microchip PIC16F and entirely written in Assembly! It was a very simple system capable of showing messages using a TV with composite video input.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/texvid_working.jpeg?style=centerme&quot; alt=&quot;TEXVID&quot; /&gt;
  &lt;figcaption&gt;TEXvid - Message Generator for Composite Video based on PIC16F.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So, around the time I was finishing my first degree, I had this idea of building my own hobby CNC router. It was not an easy task in Brazil at that time (2006?) because, sometimes, we could not find the most basic parts (with a reasonable price). Also, I had just got married, moved to a new house and I didn’t have proper tools. This time I had a camera, but I didn’t have time to properly document it and I thought I would do it when I had a better design, etc. This first machine was built using drawer slides, since I could not afford real ball bearing guides. The only picture I have is this one:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/MyFirstRouter.jpg?style=centerme&quot; alt=&quot;Poor guys MDF CNC Router&quot; /&gt;
  &lt;figcaption&gt;My first attempt to build a CNC router.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Because it was quite hard to build things that would need to be parallel, I started looking for another design paradigm. The inspiration came from an old Elektor magazine where they presented a &lt;a href=&quot;https://youtu.be/K7yomRmN52Q&quot;&gt;CNC machine using polar coordinates&lt;/a&gt;. That seemed perfect, because it would be easier to build my router using the tools I had available. However, the Elektor version had some caveats: it was necessary some gears to increase the resolution. Since I didn’t have any gearbox, I developed a simple microstep driver (again using the PIC16F) to increase the resolution. I had a digital camera, but because my life was very busy and I was moving again to a new place, I saved only this picture:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/MyRadialRouter.jpg?style=centerme&quot; alt=&quot;My polar or radial router&quot; /&gt;
  &lt;figcaption&gt;My polar or radial router: it was supposed to become, one day, a PCB engraver.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After I graduated (January 2006), I started working full time in my family’s engineering company. The company doesn’t exist anymore, but I still keep the old website online (&lt;a href=&quot;http://azamec.com&quot;&gt;Azamec.com&lt;/a&gt;). Actually, I started working with my father when I was a lot younger than that. My father was not a big fan of computers, so he would always buy the computers and teach me how to use them - this way I was his computer operator. I loved that because I always had cool computers (and I could also use them for games!). Here are some videos of the equipments we produced:&lt;/p&gt;

&lt;div class=&quot;video-container&quot; align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/UOWg353JbDk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;video-container&quot; align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/UQGq369V3AI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;video-container&quot; align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Wz_oYNcNnos&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;video-container&quot; align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/oHGCXv1XYxI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In 2011, I started my Master’s degree in Electrical Engineering / Automation. At the beginning, I was planing to do my dissertation on non-destructive testing (NDT) based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Barkhausen_effect&quot;&gt;Barkhausen effect&lt;/a&gt;. I developed quite a few things including special sensor coils and a super-high-gain-and-low-noise amplifier. However, lack of equipments and “destiny” forced me to change from Barkhausen noise to &lt;a href=&quot;http://ricardodeazambuja.com/publications/&quot;&gt;wireless power transmission&lt;/a&gt;. And, one more time, I did not save pictures or the schematics from most of the things I developed. In fact, from my master’s I have the papers and very few photographs. It was a two years degree where the first year was dedicated to attend modules from the graduate school. I attended Linear Systems, Optimization, Stochastic Processes, Instrumentation, Design of Experiments and Advanced DSP. For Optimization, we manually developed a lot of algorithms in Matlab (and I have nothing saved). Design of Experiments had a initial part on Monte Carlo methods and a second one on the design of experiments itself (yep, nothing saved again!). Stochastic Processes, Advanced DSP and Linear Systems were more traditional modules, however we developed quite a few things in Matlab, but I don’t have a line of code saved. The only pictures I could find after asking a lot of different things to my Gmail account were the ones from a special shaker we developed using an unbalanced motor and one from my workbench while I was executing the experiments for my master’s thesis.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/shaker_1.png?style=centerme&quot; alt=&quot;My polar or radial router&quot; /&gt;
  &lt;figcaption&gt;Shaker - it was strong and big enough that a person could hop on!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/shaker_2.png?style=centerme&quot; alt=&quot;My polar or radial router&quot; /&gt;
  &lt;figcaption&gt;Shaker - details of the eccentric load attached to a DC motor to vary the frequency of oscillation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/shaker_3.png?style=centerme&quot; alt=&quot;My polar or radial router&quot; /&gt;
  &lt;figcaption&gt;Shaker - Springs.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/public/images/ExperimentoBancada.jpg?style=centerme&quot; alt=&quot;My polar or radial router&quot; /&gt;
  &lt;figcaption&gt;My workbench, at home, during the final stage of my Master's degree.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’ve only really learned about sharing and publishing things online during my Ph.D, but not at the first year! I don’t know why, but I had this silly feeling that I should not publish my work online. I only managed to learn when I finished the first version of my &lt;a href=&quot;https://github.com/ricardodeazambuja/BEE&quot;&gt;Spiking Neural Network simulator&lt;/a&gt; and started using Github. Now, I really don’t care if it is unfinished, badly documented or it’s simply not such a great piece of code, I publish everything online as soon as possible :smiley:  &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hello, World!</title>
   <link href="http://localhost:4000/main/2016/11/13/hello-world/"/>
   <updated>2016-11-13T00:00:00-05:00</updated>
   <id>http://localhost:4000/main/2016/11/13/hello-world</id>
   <content type="html">&lt;p&gt;This is my first attempt using &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. It all started because my colleague, Massimiliano, showed me his brand new Jekyll based &lt;a href=&quot;http://mpatacchiola.github.io/&quot;&gt;website&lt;/a&gt;. Actually, I’d tried Jekyll before, but when I read the &lt;a href=&quot;https://jekyllrb.com/docs/installation/&quot;&gt;first lines&lt;/a&gt; asking me to install &lt;a href=&quot;https://www.ruby-lang.org/en/&quot;&gt;Ruby&lt;/a&gt;, &lt;a href=&quot;https://rubygems.org&quot;&gt;RubyGems&lt;/a&gt;, &lt;a href=&quot;https://nodejs.org/en/&quot;&gt;NodeJS&lt;/a&gt; and then install &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;, I instantaneously gave up. I was way too used to deploy and admin WordPress websites, so I could not accept the idea of not being able to have a web-only system. Luckily, Github does everything and it’s NOT necessary to install anything on you computer to use Jekyll with Github!&lt;/p&gt;

&lt;p&gt;My plan, for the future, is to keep this website updated with what I’m currently doing (and, maybe, try to document some old projects too).&lt;/p&gt;

&lt;p&gt;And, finally, here it is:&lt;/p&gt;
&lt;div class=&quot;message&quot;&gt;
  Hello, World!
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;This website was created using the &lt;a href=&quot;https://github.com/poole/lanyon&quot;&gt;Lanyon theme&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 

</feed>
