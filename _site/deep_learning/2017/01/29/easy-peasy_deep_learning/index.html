<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta property="fb:app_id" content="1545543408794841" />

  <title>
    
      Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1 &middot; Ricardo's Place
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="/public/css/images.css">
  <link rel="stylesheet" href="/public/css/extras.css">

  <!-- Icons -->
  <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">-->
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


  <!-- <base target="_blank"> -->
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<!-- <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" > -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Here I'm trying to present, in a kind of organised way, the things I'm doing, I've done, I'm planning to do or just dreaming about...</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/" target="_self">Home</a>

    

    
    
      
        
          <a class="sidebar-nav-item" href="/about/" target="_self">About...</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/publications/" target="_self">My Publications</a>
        
      
    

    <a class="sidebar-nav-item" href="https://scholar.google.co.uk/citations?user=F8AFA4gAAAAJ">Google Scholar Profile</a>
    <a class="sidebar-nav-item" href="https://github.com/ricardodeazambuja">GitHub Repositories</a>
    <a class="sidebar-nav-item" href="https://uk.linkedin.com/in/ricardodeazambuja">LinkedIn Profile</a>
    <a class="sidebar-nav-item" href="https://www.youtube.com/c/RicardodeAzambuja">My Youtube Channel</a>
    <a class="sidebar-nav-item" href="http://www.thingiverse.com/ricardodeazambuja/designs">Thingverse Profile</a>
    <!--<a class="sidebar-nav-item" href="/public/azambuja-cv-last_site.pdf">Curriculum Vitae (PDF file)</a>-->

    <span class="sidebar-nav-item">Currently v0.0.0.alpha.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      © 2018. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" target="_self">Ricardo's Place</a>
            <small>Robotics, machine learning, or simply random thoughts!</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1</h1>
  <span class="post-date">29 Jan 2017</span>
      <p><a target="_blank" href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a>… wow… this is “the” <a target="_blank" href="http://fortune.com/ai-artificial-intelligence-deep-machine-learning/">hot topic</a> since, at least, some good years ago! I’ve attended a few seminars and workshops about deep learning, nevertheless I’ve never tried to code something myself - until now! - because I had always another <a target="_blank" href="http://www.tastefullyoffensive.com/2013/09/the-12-types-of-procrastnators.html">priority</a>. Also, I have to admit, I thought it was a lot harder and it would need much more time to be able to run anything that was not simply a sample code.</p>

<figure>
  <img src="http://localhost:4000/public/images/dog_vs_cat_test_set_start.png?style=centerme" alt="Classification of Dogs and Cats">
  <figcaption>Example of classification results.</figcaption>
</figure>

<p>I’m always forgetting things, so I like to take notes as if I was teaching a toddler. Consequently, this post was designed to remember myself when I forget how to use Keras <img class="emoji" title=":expressionless:" alt=":expressionless:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f611.png" height="20" width="20">.</p>

<p>All the things I’ll explain below will only make sense if you know what is a <a target="_blank" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a> and <a target="_blank" href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feedforward neural network</a> as well. In case you don’t, no worries, Google is your friend <img class="emoji" title=":stuck_out_tongue_winking_eye:" alt=":stuck_out_tongue_winking_eye:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f61c.png" height="20" width="20">.</p>

<div class="message">
  Keras is a high-level neural networks library for TensorFlow and Theano. I would call it a Python wrapper that hides the extra details necessary to create neural networks... simplifying our life!
</div>

<p>Since I’ve just learned how <a target="_blank" href="http://blog.winddweb.info/implement-github-like-checkbox">to create Github Markdown check boxes</a>, let’s write down an outline of what we want to achieve at the end:</p>

<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Convince ourselves learning <a target="_blank" href="https://keras.io/">Keras</a> is a nice investment!</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Create our very own first deep neural network (ok, not <em>that</em> deep) applying it to a well known task.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Show off by modifying the previous example using a convolutional layer.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Enjoy our time because when you work on something you like, it is not work anymore!</li>
</ul>

<!--more-->

<p>Am I going to reinvent the wheel? Hopefully not! I will use my very strong <a target="_blank" href="https://en.wiktionary.org/wiki/Google-fu"><em>Google-fu</em></a> to find something we can reuse. The <strong>first result</strong> Google gave me was <a target="_blank" href="http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/">this</a>. The <a target="_blank" href="http://www.pyimagesearch.com">pyimagesearch</a> website is a very good source of things related to image processing, but, I’ll have to admit, I don’t like the way the guy deals with his readers <del>forcing</del> pushing them to use his own library and to subscribe to be able to download source code… however, he is sharing knowledge and this is a good thing <img class="emoji" title=":relieved:" alt=":relieved:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f60c.png" height="20" width="20">. My intention here is to partially follow his steps with some changes introduced because I thought were useful or just a matter of personal taste <img class="emoji" title=":satisfied:" alt=":satisfied:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f606.png" height="20" width="20"> (I’m using a lot of emojis because I’ve just found this <a target="_blank" href="http://www.webpagefx.com/tools/emoji-cheat-sheet/">emoji cheat sheet</a>).</p>

<p>By the way, I’m supposing <a target="_blank" href="https://keras.io/">Keras</a> and <a target="_blank" href="http://deeplearning.net/software/theano/">Theano</a> (or <a target="_blank" href="https://www.tensorflow.org/">TensorFlow</a>) are already installed, up and running. My personal experience tells me Theano is easier to install than TensorFlow, but, maybe, I was just unlucky/lucky <img class="emoji" title=":sunglasses:" alt=":sunglasses:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f60e.png" height="20" width="20">.</p>

<p>I’m using Theano on a laptop that has a GeForce GT 750M GPU, but I have found one caveat related to the amount of memory available (<del>the system shares its main memory with the GPU</del> <em>I’ve found out it actually has 2GB of dedicated GDDR5 memory</em>). To have more control, I’ve created a file in my user’s home directory (<code class="highlighter-rouge">~/</code>) called <em>.theanorc</em> with this content:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[global]
floatX=float32
device = gpu0

[lib]
cnmem = .7
</code></pre></div></div>

<p>Theano can run on CPU or CPU+GPU. It will autonomously choose the CPU only mode if the GPU is not available. However, my system has a GPU and I was not able to activate its use on Theano. That’s the reason why I created the <em>.theanorc</em> file. The line <code class="highlighter-rouge">device = gpu0</code> forces Theano to use the GPU (if you have more than one, maybe it will not be <code class="highlighter-rouge">gpu0</code>) and the line <code class="highlighter-rouge">cnmem = .7</code> sets the amount of memory used by <a target="_blank" href="http://deeplearning.net/software/theano/library/config.html#config.config.lib.cnmem">CNMeM</a>. If you don’t have enough memory available (<del>laptops usually share the main memory with the GPU</del>), it will give you an error message. Setting <code class="highlighter-rouge">cnmem = 0</code> disables it.</p>

<p>Before we start <em>deep learning</em>, we are going to need the data set from <a target="_blank" href="https://www.kaggle.com/c/dogs-vs-cats">Kaggle Dogs vs. Cats</a>. This data set has 25000 images divided into training (12500) and testing (12500) sets. The training one has the filenames like these examples: <code class="highlighter-rouge">cat.3141.jpg</code> and <code class="highlighter-rouge">dog.3141.jpg</code>, while in the testing set the files are only a number with the <code class="highlighter-rouge">.jpg</code> extension. I’m <strong>not</strong> trying to beat a state of art algorithm (<a target="_blank" href="http://xenon.stanford.edu/~pgolle/papers/dogcat.pdf">not even an old one</a>), but only to learn how to use Keras. Our network should be able to gives us an answer something like a <code class="highlighter-rouge">0</code> if it is a cat and <code class="highlighter-rouge">1</code> if it is a dog.</p>

<p>If you have a look at the images you just downloaded, you will notice they’re not all the same size. Later, we will need resize them (our network accepts only a fixed size input) and simplify things or it will take ages to train, run, etc. I will consider the data sets are, now, installed in two directories (folders, for the young ones) named: <code class="highlighter-rouge">train</code> and <code class="highlighter-rouge">test1</code>. We will need to read the images and store their filenames as well. Since we will have a loop going on, the images will pass through a downsampling to reduce their sizes too. I will use <code class="highlighter-rouge">scipy.misc.imresize</code> because, IMHO, it’s a lot easier to install <a target="_blank" href="https://pypi.python.org/pypi/Pillow/2.2.1">PIL/Pillow</a> than <a target="_blank" href="https://www.google.co.uk/webhp?q=installing+opencv+python">OpenCV</a> <img class="emoji" title=":innocent:" alt=":innocent:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f607.png" height="20" width="20">.</p>

<div class="message">
  Summarizing: it's necessary to install Theano (or TensorFlow), Keras, Scipy (Numpy) and Pillow (if it was not automatically installed with Scipy).
</div>

<p>Even though it’s not a good practice, I will import the packages only when they are necessary. Python is just fine with that and it is so clever that it will not import twice the same thing. At the end I will supply a file with the whole source code and, then, the imports will be all at the top (if I don’t miss some…<img class="emoji" title=":cold_sweat:" alt=":cold_sweat:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f630.png" height="20" width="20">).</p>

<p>Another <em>small</em> detail: the neural network we will create here only accepts one dimensional (1D) vector (or list or array, you choose the name). For that reason, we will flatten (transform it into a 1D thing) after resizing (downsampling) the images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The package 'os' will be necessary to find the current</span>
<span class="c"># working directory os.getcwd() and also to list all</span>
<span class="c"># files in a directory os.listdir().</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c"># As I've explained somewhere above, Scipy will help us</span>
<span class="c"># reading (using Pillow) and resizing images.</span>
<span class="kn">import</span> <span class="nn">scipy.misc</span>

<span class="c"># Returns the current working directory</span>
<span class="c"># (where the Python interpreter was called)</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>

<span class="c"># The path separator used by the OS:</span>
<span class="n">sep</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span>

<span class="c"># This is the directory where the training images are located</span>
<span class="n">dirname</span> <span class="o">=</span> <span class="s">"train"</span>

<span class="c"># Generates a list of all images (actualy the filenames) from the training set,</span>
<span class="c"># but it will also include the full path</span>
<span class="n">imagePaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">path</span><span class="o">+</span><span class="n">sep</span><span class="o">+</span><span class="n">dirname</span><span class="o">+</span><span class="n">sep</span><span class="o">+</span><span class="n">filename</span>
                  <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="o">+</span><span class="n">sep</span><span class="o">+</span><span class="n">dirname</span><span class="p">)]</span>
</code></pre></div></div>

<p>Now, we should have a list (<code class="highlighter-rouge">imagePaths</code>) with all the filenames (full path) for the images in the training set. The obvious next step is to read all those images and convert to something readable to our <em>deep net</em>. Reading things from a hard drive is always slow and I don’t like to waste time, therefore we will use <em>parallel processing powers</em> from Python <a target="_blank" href="https://docs.python.org/2/library/multiprocessing.html">multiprocessing</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># To speed up things, I will use parallel computing!</span>
<span class="c"># Pool lets me use its map method and select the number of parallel processes.</span>
<span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span>

<span class="k">def</span> <span class="nf">import_training_set</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="s">'''
    Reads the image from imagePath, resizes
    and returns it together with its label and
    original index.

    '''</span>
    <span class="n">index</span><span class="p">,</span><span class="n">imagePath</span><span class="p">,</span><span class="n">new_image_size</span> <span class="o">=</span> <span class="n">args</span>

    <span class="c"># Reads the image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">misc</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">imagePath</span><span class="p">)</span>

    <span class="c"># Split will literally split a string at that character</span>
    <span class="c"># returning a list of strings.</span>
    <span class="c"># First we split according to the os.path.sep and keep</span>
    <span class="c"># only the last item from the list gererated ([-1]).</span>
    <span class="c"># This will give us the filename.</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">imagePath</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># Then we split it again using "."  </span>
    <span class="c"># and extract the first item ([0]):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">"."</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c"># and the second item ([1]):</span>
    <span class="n">original_index</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">"."</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># Resizes the image (downsampling) to new_image_size and</span>
    <span class="c"># converts it to a 1D list (flatten).</span>
    <span class="n">input_vector</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">misc</span><span class="o">.</span><span class="n">imresize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">new_image_size</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">index</span><span class="p">,(</span><span class="n">original_index</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">input_vector</span><span class="p">))</span>

<span class="c"># The size of the resized image.</span>
<span class="c"># After we apply the flatten method, it will become a list of 32x32x3 items (1024x3).</span>
<span class="c"># (Where is the 'x3' coming from? Our images are composed of three colors!)</span>
<span class="n">new_image_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span><span class="mi">32</span>

<span class="n">number_of_parallel_processes</span> <span class="o">=</span> <span class="mi">7</span>

<span class="c"># When the map is finished, we will receive a list with tuples:</span>
<span class="c"># (index, 'category', 1D resized image)</span>
<span class="c"># There's no guarantee about the ordering because they are running in parallel.</span>
<span class="n">ans</span> <span class="o">=</span> <span class="n">Pool</span><span class="p">(</span><span class="n">number_of_parallel_processes</span><span class="p">)</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">import_training_set</span><span class="p">,[(</span><span class="n">i</span><span class="p">,</span><span class="n">img</span><span class="p">,</span><span class="n">new_image_size</span><span class="p">)</span>
                                                                  <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">imagePaths</span><span class="p">)])</span>

<span class="c"># Because import_training_set returns a tuple like this:</span>
<span class="c"># (index,(original_index,label,input_vector))</span>
<span class="c"># and index is unique, we can convert to a dictionary</span>
<span class="c"># to solve our problem with unordered items:</span>
<span class="n">training_set</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>

<span class="c"># Gives a hint to the garbage collector...</span>
<span class="k">del</span> <span class="n">ans</span>
</code></pre></div></div>
<p>Our poor neural network can only understand numbers, so we will convert our labels (<code class="highlighter-rouge">cat</code> and <code class="highlighter-rouge">dog</code>) to integers, but the Python code will be a little bit more generic. The result will be: <code class="highlighter-rouge">{'cat': 1, 'dog': 0}</code>. Another nice thing to do is the <a target="_blank" href="http://stackoverflow.com/a/4674770">normalization of input variables</a> (<a target="_blank" href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/">the source referenced on stackoverflow</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Let's imagine we don't know how many different labels we have.</span>
<span class="c"># (we know, they are 'cat' and 'dog'...)</span>
<span class="c"># A Python set can help us to create a list of unique labels.</span>
<span class="c"># (i[0]=&gt;filename index, i[1]=&gt;label, i[2]=&gt;1D vector)</span>
<span class="n">unique_labels</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">training_set</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>

<span class="c"># With a list of unique labels, we will generate a dictionary</span>
<span class="c"># to convert from a label (string) to a index (integer):</span>
<span class="n">labels2int</span> <span class="o">=</span> <span class="p">{</span><span class="n">j</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">)}</span>


<span class="c"># Creates a list with labels</span>
<span class="c"># (ordered according to the dictionary index)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels2int</span><span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">training_set</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
</code></pre></div></div>

<p>Ok, at the very beginning I asked about Multilayer Perceptron (MLP), Feedforward Neural Network (F… NN), etc. So, this type of NN, typically, works with <a target="_blank" href="https://en.wikipedia.org/wiki/Floating_point">real valued numbers</a> essentially doing multiplications (also additions). If you enter a value zero (and there’s no <a target="_blank" href="http://stackoverflow.com/a/2499936">bias</a>) it will return you zero. Why am I telling you this? I thought it was something important <img class="emoji" title=":satisfied:" alt=":satisfied:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f606.png" height="20" width="20">.</p>

<p>Just to refresh your memory, I’m working based on the <a target="_blank" href="http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/">pyimagesearch post</a>. This means I will use the <a target="_blank" href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a> loss function (<a target="_blank" href="https://keras.io/metrics/#binary_crossentropy">Keras binary_crossentropy</a>) as well, forcing us to format the labels as vectors <code class="highlighter-rouge">[0.,1.]</code> and <code class="highlighter-rouge">[1.,0.]</code> and this will be the kind of answer our network will give us at the end.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Necessary for to_categorical method.</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">np_utils</span>

<span class="c"># Instead of '1' and '0', the function below will transform our labels</span>
<span class="c"># into vectors [0.,1.] and [1.,0.]:</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np_utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>When you train a NN you need to have a way to test if it is learning. This is accomplished by reserving a piece of the training set for testing the network. Nevertheless, you can’t just save 75% of the images… you do it <a target="_blank" href="http://stats.stackexchange.com/questions/248048/neural-networks-why-do-we-randomize-the-training-set/248053">randomly</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># First, we will create a numpy array going from zero to len(training_set):</span>
<span class="c"># (dtype=int guarantees they are integers)</span>
<span class="n">random_selection</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_set</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

<span class="c"># Then we create a random state object with our seed:</span>
<span class="c"># (the seed is useful to be able to reproduce our experiment later)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">12345</span>
<span class="n">rnd</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c"># Finally we shuffle the random_selection array:</span>
<span class="n">rnd</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">random_selection</span><span class="p">)</span>

<span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span> <span class="c"># we will use 25% for testing purposes</span>

<span class="c"># Breaking the code below to make it easier to understand:</span>
<span class="c"># =&gt; training_set[i][-1]</span>
<span class="c"># Returns the 1D vector from item 'i'.</span>
<span class="c"># =&gt; training_set[i][-1]/255.0</span>
<span class="c"># Normalizes the input values from 0. to 1.</span>
<span class="c"># =&gt; random_selection[:int(len(training_set)*(1-test_size))]</span>
<span class="c"># Gives us the first (1-test_size)*100% of the shuffled items</span>
<span class="n">trainData</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_set</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mf">255.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">random_selection</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">test_size</span><span class="p">))]]</span>
<span class="n">trainData</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trainData</span><span class="p">)</span>

<span class="n">trainLabels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">random_selection</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">test_size</span><span class="p">))]]</span>
<span class="n">trainLabels</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trainLabels</span><span class="p">)</span>

<span class="n">testData</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_set</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mf">255.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">random_selection</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">test_size</span><span class="p">))]]</span>
<span class="n">testData</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testData</span><span class="p">)</span>

<span class="n">testLabels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">random_selection</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">test_size</span><span class="p">))]]</span>
<span class="n">testLabels</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testLabels</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, we will create our <em>deep neural network</em> model! I’ll not explain everything because <a target="_blank" href="https://keras.io/getting-started/sequential-model-guide/">Keras guide to Sequential model</a> is already easy-peasy. This neural network will have three layers, all the layer will use Keras <a target="_blank" href="https://keras.io/layers/core/#dense">Dense</a> type. This layer type accepts a lot of arguments, but, for the first layer, we will only pass three: output dimension (output_dim), <a target="_blank" href="https://keras.io/getting-started/sequential-model-guide/#specifying-the-input-shape">input dimension (input_dim)</a>, <a target="_blank" href="https://keras.io/initializations/">initialization (init)</a>. After this first layer, we will add an <a target="_blank" href="https://keras.io/activations/">Activation</a> as <a target="_blank" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear unit (ReLU)</a>. The second layer will only need two arguments (output_dim and initialization), since it can do automatic shape inference. The last one will have only two inputs because we want to classify between two classes (binary) and the <a target="_blank" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> activation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Just creates our Keras Sequential model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c"># The first layer will have a uniform initialization</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">))</span>
<span class="c"># The ReLU 'thing' :)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># Now this layer will have output dimension of 384</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># Because we want to classify between only two classes (binary), the final output is 2</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"softmax"</span><span class="p">))</span>
</code></pre></div></div>

<p>After finishing the network creation, we need to <a target="_blank" href="https://keras.io/getting-started/sequential-model-guide/#compilation">compile</a> our masterpiece. Keras has a lot of options for <a target="_blank" href="https://keras.io/optimizers/">optimizers</a>, so let’s try more than one and compare. The first one will be the example from Keras guide (Root Mean Square Propagation).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># for a binary classification problem</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<p>After compilation comes <a target="_blank" href="https://keras.io/getting-started/sequential-model-guide/#training">training</a> (the <code class="highlighter-rouge">.fit</code> method).  Training has even more <a target="_blank" href="https://keras.io/models/sequential/#fit">arguments</a> than the compilation step! I’m not going to talk about all them, but these ones (from Keras documentation):</p>

<ul>
  <li>
<strong>batch_size:</strong> integer, the number of samples per gradient update.</li>
  <li>
<strong>nb_epoch:</strong> integer, the number of epochs to train the model.</li>
  <li>
<strong>verbose:</strong> 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.</li>
  <li>
<strong>initial_epoch:</strong> epoch at which to start training (useful for resuming a previous training run).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span> <span class="n">trainLabels</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testData</span><span class="p">,</span> <span class="n">testLabels</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test score:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>
<p>Using the Root Mean Square Propagation, our network was basically random (accuracy 51.1520%). Let’s try another <a target="_blank" href="https://keras.io/optimizers/">optimizer</a>, the first one from Keras list: <a target="_blank" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Dradient Descent</a> (SDG). If you are using IPython, don’t forget to recreate your model before you try to compile it again.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">"binary_crossentropy"</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span> <span class="n">trainLabels</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testData</span><span class="p">,</span> <span class="n">testLabels</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test score:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>Wow! Now the accuracy jumped to 77.2480%! That said, remember the <a target="_blank" href="http://xenon.stanford.edu/~pgolle/papers/dogcat.pdf">2008 paper</a> already got 82.7% <img class="emoji" title=":disappointed_relieved:" alt=":disappointed_relieved:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f625.png" height="20" width="20">. Let’s try one last optimizer to see if something happens, this time it will be the <a target="_blank" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad">Adaptive Gradient Algorithm</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adagrad</span>

<span class="n">ada</span> <span class="o">=</span> <span class="n">Adagrad</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"binary_crossentropy"</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span> <span class="n">trainLabels</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testData</span><span class="p">,</span> <span class="n">testLabels</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test score:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>The accuracy on the test data was 67.1840%. Probably there’s a reason why SDG is the first one on Keras documentation about optimizers. In addition, we are not fiddling with the parameters, e.g. nb_epoch. According to this <a target="_blank" href="https://www.kaggle.com/c/dogs-vs-cats/forums/t/6845/example-use-decaf-via-nolearn-for-94-accuracy">discussion</a>, using DeCAF (now <a target="_blank" href="http://caffe.berkeleyvision.org/">Caffe</a>) via <a target="_blank" href="https://github.com/dnouri/nolearn">nolearn</a> the accuracy was 94%!</p>

<p>After all our hard work, you should save your model. Keras models have methods for saving and loading a model. Everything is nicely explained on the <a target="_blank" href="https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model">F.A.Q.</a> and the simplest way is <code class="highlighter-rouge">model.save('my_model.h5')</code> to save and <code class="highlighter-rouge"><span class="k">model</span> <span class="p">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">'my_model.h5'</span><span class="p">)</span></code> to load everything (the architecture , the weights, the training configuration - loss, optimizer - and the state of the optimizer).</p>

<p>And then I decided to fiddle with the parameters using the SDG optimizer. What did I do? I simply changed <code class="highlighter-rouge">nb_epoch=20</code> and, <em>voilà</em>, the accuracy went up to… 97.74%! You can find the saved model <a target="_blank" href="https://github.com/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/my_97perc_acc.h5?raw=true">here</a>. But, gosh, why was it so good now? In order to try to understand what happened, we must go back to where we defined our neural network. For the first two layers, we added this argument: <code class="highlighter-rouge">init="uniform"</code>. Therefore, those layers had their weights randomly assigned (<a target="_blank" href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a>). A further reason could be some <strong>crazy</strong>  <a target="_blank" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> situation because the number of epochs were reduced from 50 to 20 and overfitting would usually occur when you train for too long. In the future, I will try to fight overfitting using the <a target="_blank" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout</a> technique <a target="_blank" href="https://keras.io/layers/core/#dropout">already available in Keras</a>.</p>

<p>Here are the results from the best neural network (the 97.74% accuracy one) using images from the test set (the 25% randomly chosen images from the directory train):</p>

<figure>
  <img src="http://localhost:4000/public/images/dog_vs_cat_test_set.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Testing the Neural Network against images from the test set.</figcaption>
</figure>

<p><strong>UPDATE (11/02/2017): I was not normalizing the images before sending them to the network!</strong> I was reading again this post to start the <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning_one_and_a_half/"><em>Part 1½</em></a> when I realized the outputs were always saturating (1 or 0) and then I noticed the problem with the lack of normalization<img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">.</p>

<p>And using images never seen before (directory test1):</p>
<figure>
  <img src="http://localhost:4000/public/images/dog_vs_cat_novel.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Testing the Neural Network against images it has never seen before.</figcaption>
</figure>

<p>Ok, let’s see what we have achieved so far:</p>

<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Convince ourselves learning <a target="_blank" href="https://keras.io/">Keras</a> is a nice investment!</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Create our very own first deep neural network (ok, not <em>that</em> deep) applying it to a well known task.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Show off by modifying the previous example using a convolutional layer.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Enjoy our time because when you work on something you like, it is not work anymore!</li>
</ul>

<p>As promised, <a target="_blank" href="http://nbviewer.jupyter.org/github/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/Keras%20Cats%20and%20Dogs%20-%20normal%20deep%20net%20(not%20so%20deep).ipynb">here</a> you can visualize (or download) a <a target="_blank" href="https://ipython.org/notebook.html">Jupyter (IPython) notebook</a> with all the source code and something else.</p>

<p>In the next post, we will see how to convert our simple <em>deep</em> neural network to a convolutional neural network. Cheers!</p>

<p><strong>UPDATE (15/02/2017): Part 1½ is available <a target="_blank" href="http://localhost:4000/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/">here</a>.</strong></p>

<!---
<div class="message">
  This is a draft... yep, I'm learning how to use Jekyll and I do test things on the production website :bowtie:
</div>
--->

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      
    
      
    
      
        
          
            <li>
              <h3>
              <a class="" href="target=%22_self%22">
                
              </a>
             </h3>
            </li>
          
        
          
            <li>
              <h3>
              <a class="" href="/deep_learning/2017/03/05/easy-peasy_conv_deep_learning_two/" target="_self">
                Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 2
              </a>
             </h3>
            </li>
          
            <li>
              <h3>
              <a class="" href="/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/" target="_self">
                Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1½
              </a>
             </h3>
            </li>
          
            <li>
              <h3>
              <a class="" href="/deep_learning/2017/01/29/easy-peasy_deep_learning/" target="_self">
                Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1
              </a>
             </h3>
            </li>
          
        
      
    
      
    
      
    
      
    
      
    
  </ul>
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/projects/2016/12/19/old_projects/" target="_self">Older</a>
  
  
    <a class="pagination-item newer" href="/jupyter_notebooks/2017/02/10/Jupyter_notebook_remotelly/" target="_self">Newer</a>
  
</div>





<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/deep_learning/2017/01/29/easy-peasy_deep_learning"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//ricardos-place.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a target="_blank" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>



      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
