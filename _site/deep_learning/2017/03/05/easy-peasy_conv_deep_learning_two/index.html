<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta property="fb:app_id" content="1545543408794841" />

  <title>
    
      Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 2 &middot; Ricardo's Place
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="/public/css/images.css">
  <link rel="stylesheet" href="/public/css/extras.css">

  <!-- Icons -->
  <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">-->
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


  <!-- <base target="_blank"> -->
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<!-- <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" > -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Here I'm trying to present, in a kind of organised way, the things I'm doing, I've done, I'm planning to do or just dreaming about...</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/" target="_self">Home</a>

    

    
    
      
        
          <a class="sidebar-nav-item" href="/about/" target="_self">About...</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/publications/" target="_self">My Publications</a>
        
      
    

    <a class="sidebar-nav-item" href="https://scholar.google.co.uk/citations?user=F8AFA4gAAAAJ">Google Scholar Profile</a>
    <a class="sidebar-nav-item" href="https://github.com/ricardodeazambuja">GitHub Repositories</a>
    <a class="sidebar-nav-item" href="https://uk.linkedin.com/in/ricardodeazambuja">LinkedIn Profile</a>
    <a class="sidebar-nav-item" href="https://www.youtube.com/c/RicardodeAzambuja">My Youtube Channel</a>
    <a class="sidebar-nav-item" href="http://www.thingiverse.com/ricardodeazambuja/designs">Thingverse Profile</a>
    <!--<a class="sidebar-nav-item" href="/public/azambuja-cv-last_site.pdf">Curriculum Vitae (PDF file)</a>-->

    <span class="sidebar-nav-item">Currently v0.0.0.alpha.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      © 2018. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" target="_self">Ricardo's Place</a>
            <small>Robotics, machine learning, or simply random thoughts!</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 2</h1>
  <span class="post-date">05 Mar 2017</span>
      <p>This is the continuation (finally!), or the <em>Part 2</em>, of the <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/">“Easy-peasy Deep Learning and Convolutional Networks with Keras”</a>. This post should be something self-contained, but you may enjoy reading <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/"><em>Part 1</em></a> and <a target="_blank" href="http://localhost:4000/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/"><em>Part 1½</em></a>… it’s up to you.</p>

<p>Around one week ago, I’d attended a <a target="_blank" href="https://sites.google.com/site/cudatraining/">CUDA workshop</a> presented (or should I say <a target="_blank" href="https://books.google.com/ngrams/graph?content=workshop+presented%2Cworkshop+conducted&amp;year_start=1800&amp;year_end=2000&amp;corpus=15&amp;smoothing=3&amp;share=&amp;direct_url=t1%3B%2Cworkshop%20presented%3B%2Cc0%3B.t1%3B%2Cworkshop%20conducted%3B%2Cc0">conducted</a>?) by my friend <a target="_blank" href="https://www.facebook.com/AmityFarm/">Anthony Morse</a> and I’m still astounded by <a target="_blank" href="https://developer.nvidia.com/digits">DIGITS</a>. So, during the workshop, I had some interesting ideas to use on this post!</p>

<p>The first thing I thought when I read (or heard?) for the first time the name <strong>Convolutional Neural Network</strong> was <em>a bunch of <a target="_blank" href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">filters</a></em> (<a target="_blank" href="https://docs.gimp.org/en/plug-in-convmatrix.html">Gimp would agree with me</a>). I’m an Electrical Engineer and, for most of us (Electrical Engineers), convolutions start as nightmares and, gradually, become our almighty super weapon after a module like <a target="_blank" href="https://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/">Signal and Systems</a>.</p>

<p>Let’s start with something easy… a <a target="_blank" href="https://www.youtube.com/watch?v=puxHUGpuOVw">video</a>! Below, you can observe, step-by-step, what happens when a 2D convolution (think about a <a target="_blank" href="https://en.wikipedia.org/wiki/Edge_detection">filter</a> that detects, or enhances, edges) is applied to an image:</p>

<div class="video-container" align="center">
<iframe width="854" height="480" src="https://www.youtube.com/embed/puxHUGpuOVw" frameborder="0" allowfullscreen=""></iframe>
</div>
<p><br></p>

<!--more-->

<p>The <em>red’ish</em> 3x3 square moving around the cat’s face is where the kernel (an <a target="_blank" href="https://en.wikipedia.org/wiki/Kernel_(image_processing)#Details">edged detector</a>) is instantaneously applied. If the terms I’m using here are giving you <a target="_blank" href="http://dictionary.cambridge.org/dictionary/english/goosebumps">goosebumps</a>, try to read this first <a target="_blank" href="http://www.wildml.com/deep-learning-glossary/">deep learning glossary</a> first. When I say <em>applied</em> I mean an <a target="_blank" href="http://www.scipy-lectures.org/intro/numpy/operations.html#id2">elementwise</a> multiplication where the result is presented at the bottom (small square, really pixelated or 3x3 if you prefer). The picture, on the right hand side, is the sum of the values (you can visualize them on the small square figure at the bottom) at that instant (but the scales now are different, final result uses <a target="_blank" href="https://en.wikipedia.org/wiki/Absolute_value">absolute values</a>). If you, like me, thinks my explanation above is very poor and you want to understand what is really happening (e.g. Why does the original image have a black border?), I would suggest you to have a look on these websites: <a target="_blank" href="http://cs231n.github.io/convolutional-networks">CS231n</a>, <a target="_blank" href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">Colah’s</a> and <a target="_blank" href="http://intellabs.github.io/RiverTrail/tutorial/">Intel Labs’ River Trail project</a>. The figure below, taken from Intel’s website, is (<a target="_blank" href="https://en.wiktionary.org/wiki/IMHO">IMHO</a>) <em>the killer</em> explanation:</p>

<figure>
  <img src="http://localhost:4000/public/images/convolution_intellabs.png?style=centerme" alt="What happens when we do a 2D convolution">
  <figcaption>This is exactly what I did to create the video. <i>Image from <a target="_blank" href="http://intellabs.github.io/RiverTrail/tutorial/">Intel Labs' River Trail project</a></i>.</figcaption>
</figure>

<p>Nevertheless, this post is supposed to be <em>easy-peasy®</em> style and I will not change it now. My definition for <a target="_blank" href="https://en.wikipedia.org/wiki/Convolution">convolution</a> is: a close relative of <a target="_blank" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> (pay attention to <em>τ</em> and <em>t</em> if you decide to follow the previous link and have a closer look at the initial animation I’ve presented) and very good friend of <a target="_blank" href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> (try to <a target="_blank" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html">flatten</a> things and it will be easier to spot it) - even the symbols have a strong resemblance. <em>Improving</em> even further my <em>easy-peasy®</em> description, another way to describe what a convolution is would be by saying it is like rubbing one function (our <a target="_blank" href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a>) against another one and taking note of the result while you do it <img class="emoji" title=":satisfied:" alt=":satisfied:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f606.png" height="20" width="20">.</p>

<p>I’m starting to hate this idea of having this <em>outline thing</em>… but I will try to keep using it! If I got it right, after reading (writing, in my situation) everything and trying to figure out where all things came from and why, we should be able to:</p>

<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Undoubtedly convince ourselves <a target="_blank" href="https://keras.io/">Keras</a> is cool!</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Create our first very own <strong>Convolutional</strong> Neural Network.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Understand there’s no magic! (but if we just had <a target="_blank" href="https://en.wikipedia.org/wiki/Infinite_monkey_theorem">enough monkeys</a>…)</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Get a job on Facebook… ok, we will need a lot more to impress <a target="_blank" href="http://yann.lecun.com/">Monsieur Lecun</a>, but this is a starting point <img class="emoji" title=":bowtie:" alt=":bowtie:" src="https://assets-cdn.github.com/images/icons/emoji/bowtie.png" height="20" width="20">.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>And, finally, enjoy our time while doing all the above things!</li>
</ul>

<p>Ok, ok, ok… let’s get back to <a target="_blank" href="https://keras.io/">Keras</a> and our <a target="_blank" href="https://www.youtube.com/watch?v=Iz-8CSa9xj8"><em>precious</em></a> Convolutional Neural Networks (<a target="_blank" href="https://en.wikipedia.org/wiki/Aka">aka</a> <strong>CNNs</strong>) <img class="emoji" title=":wink:" alt=":wink:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f609.png" height="20" width="20">.</p>

<p>Do you remember I said CNNs were just <em>a bunch of filters</em>? I was not lying or making fun of you. CNNs are a mixture between a fancy <a target="_blank" href="https://en.wikipedia.org/wiki/Convolution">filter bank</a> and <a target="_blank" href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html">Feedforward Neural Networks</a>, but everything works together with the help of our friend <a target="_blank" href="http://cs231n.github.io/optimization-2/">backpropagation</a>. Recalling from <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/">Part 1</a>, our network was initially designed only using <a target="_blank" href="https://keras.io/layers/core/#dense">Kera’s Dense layers</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Just creates our Keras Sequential model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c"># The first layer will have a uniform initialization</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">))</span>
<span class="c"># The ReLU 'thing' :)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># Now this layer will have output dimension of 384</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># Because we want to classify between only two classes (binary), the final output is 2</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"softmax"</span><span class="p">))</span>
</code></pre></div></div>

<p>And <strong>what is the problem with the classical layers?</strong> <a target="_blank" href="https://en.wikipedia.org/wiki/Connectionism">Theoretically</a>, <strong>none</strong>. One could, somehow, train a network with enough data to do the job. However, <a target="_blank" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">backpropagation is not so powerful</a> and it would probably demand networks <a target="_blank" href="http://cs231n.github.io/convolutional-networks/#overview">a lot bigger</a> (this is my <a target="_blank" href="https://en.wikipedia.org/wiki/TL;DR">TL;DR</a> explanation, so go and use your inner <a target="_blank" href="http://lifehacker.com/5940946/20-google-search-shortcuts-to-hone-your-google-fu"><em>google-fu</em></a> if you are not satisfied <img class="emoji" title=":grimacing:" alt=":grimacing:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f62c.png" height="20" width="20">).</p>

<p>CNNs are, IMHO, the engineer’s approach! They are useful because they can generate (filter out) nice features from its inputs making the life easier for <a target="_blank" href="https://keras.io/layers/core/#dense">Dense layers</a> after that. Do you remember <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning_one_and_a_half/">the last post of our <em>easy-peasy®</em> series</a>? Ok, I would not expect anybody to fully remember anything, but that post was created because I realized visualization of internal layers was super important and I will bring back one important figure from there:</p>

<figure>
  <img src="http://localhost:4000/public/images/output_trained.png?style=centerme" alt="Layers output as images">
  <figcaption>This is what my trained network from Part 1½ outputs (not the CNN!), but viewed as RGB images.</figcaption>
</figure>

<p>The first image is <em>clearly</em> a dog, but the second image (composed by the output from the first layer) is totally crazy and doesn’t look like having features to distinguish dogs from cats! So, the important bit is that I (or maybe a dense layer too) can’t even find any tiny visual clue from that image that would suggest there was a dog as input. That doesn’t mean there are no distinguishable features, but the features are hard to spot.</p>

<p>Now let’s see what happens when we use a convolutional layer:</p>

<figure>
  <img src="http://localhost:4000/public/images/conv_layer1.png?style=centerme" alt="Layers output as images">
  <figcaption>Now, our brand new CNN outputs something different from its first layer.</figcaption>
</figure>

<p>Probably, the first thing I would notice here is the number of images after the first layer. Our convolutional layer created 32, different, filtered versions of its input! Kind of cheating…</p>

<p>In order to have the above result, we need to replace, in our old model, the first <a target="_blank" href="https://keras.io/layers/core/#dense"><code class="highlighter-rouge">Dense</code></a> layer with a <a target="_blank" href="https://keras.io/layers/convolutional/#convolution2d"><code class="highlighter-rouge">Convolution2D</code></a> one:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Convolution2D</span><span class="p">(</span><span class="n">number_of_filters</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">border_mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                        <span class="n">input_shape</span><span class="o">=</span><span class="n">input_layer_shape</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s">'ConvLayer1'</span><span class="p">))</span>
</code></pre></div></div>

<p>Where:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>number_of_filters=32
kernel_size=(3,3)
</code></pre></div></div>

<p>This layer is composed by 32 filters (that is why we got 32 output images after the first layer!), each filter 3x3x3 (remember our input images are colorful <a target="_blank" href="http://stackoverflow.com/questions/25102461/python-rgb-matrix-of-an-image">RGB</a> ones, therefore they have one 3x3 matrix for each color), and it will output 32 images 32x32 because <code class="highlighter-rouge">border_mode='same'</code>. It’s crystal clear that this behaviour would lead to an <a target="_blank" href="https://en.wikipedia.org/wiki/Geometric_series">explosion</a> because the next layer would always be 32x bigger than the previous… but <strong>not</strong>! If we read the <a target="_blank" href="https://keras.io/layers/convolutional/#convolution2d">Convolution2D manual</a>, this layer receives a 4D <a target="_blank" href="http://www.physlink.com/education/askexperts/ae168.cfm">Tensor</a>, returns a 4D Tensor (no extra dimension created) and, consequently, no explosion at all because the generated weights do the magic of combining things for us <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">. If we verify the shape of the weights tensor, the first convolutional layer weights tensor has shape (32, 3, 3, 3) and the second convolutional layer has shape (8, 32, 3, 3). Remember, the input image is (3, 32, 32), the first layer has 32 filters and the second only 8.</p>

<p>Even though the explosion problem does not afflict us, I will condense the features before the last fully-connected Keras <code class="highlighter-rouge">Dense</code> layer to speed up things by reducing the total number of weights and, therefore, calculations. The solution for this is the use of a shrinking layer and the <a target="_blank" href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">pooling layer</a> is what we need. From Keras, we will use a <a target="_blank" href="https://keras.io/layers/pooling/#maxpooling2d"><code class="highlighter-rouge">Maxpooling2d</code></a> layer with <code class="highlighter-rouge">pool_size=(2,2)</code> and it will shrink our 32x32x32 output down to 32x16x16. The reduction from 32x32 to 16x16 is done by keeping the biggest value (probably that’s the reason of starting with “Max”) inside the 2x2 pool.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Convolution2D</span><span class="p">(</span><span class="n">number_of_filters</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">border_mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                        <span class="n">input_shape</span><span class="o">=</span><span class="n">input_layer_shape</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s">'ConvLayer2'</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">,</span>
                     <span class="n">name</span><span class="o">=</span><span class="s">'ConvLayer2Activation'</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span>
                       <span class="n">dim_ordering</span><span class="o">=</span><span class="s">'th'</span><span class="p">,</span>
                       <span class="n">name</span><span class="o">=</span><span class="s">'PoolingLayer2'</span><span class="p">))</span>
</code></pre></div></div>

<p>Since this <em>easy-peasy®</em> series was not designed to beat benchmarks, but mainly to learn and understand what is happening, I will add one more set of convolutional and pooling layers. Moreover, I will reduce the number of filters from 32 to 8. I really want to see changes on the internal weights, even if this generates overfitting, thus I’ve increased the epochs to 1000. Apart from the things I’ve explained here so far, everything else will be kept the same as the <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/">Part 1</a> and <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/">Part 1½</a> posts.</p>

<p>Testing our new CNN against the test set (the 25% randomly chosen images from the directory train) returned us a accuracy of… 97%! You can find the saved model <a target="_blank" href="https://github.com/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/my_convnet_SDG.h5?raw=true">here</a>.</p>

<figure>
  <img src="http://localhost:4000/public/images/dog_vs_cat_conv_test_set.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Testing the Convolutional Neural Network against images from the test set.</figcaption>
</figure>

<p>And below you can see what appears inside the CNN with the dog:</p>

<figure>
  <img src="http://localhost:4000/public/images/conv_layer1.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>First Layer internal results using the dog image.</figcaption>
</figure>

<figure>
  <img src="http://localhost:4000/public/images/conv_layer2.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Second Layer internal results using the dog image.</figcaption>
</figure>

<p>In order to help us verify the differences between dogs and cats, here are the images where a cat was the input:</p>

<figure>
  <img src="http://localhost:4000/public/images/conv_layer1_cat.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>First Layer internal results using the cat image.</figcaption>
</figure>

<figure>
  <img src="http://localhost:4000/public/images/conv_layer2_cat.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Second Layer internal results using the cat image.</figcaption>
</figure>

<p>After all that, I was not 100% sure if Keras was really doing convolution or cross-correlation. Instead of searching for an answer, I decided to verify it myself. First, I save the trained weights:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer_name</span> <span class="o">=</span> <span class="s">'ConvLayer1'</span>
<span class="n">weights_conv1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">bias_conv1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">layer_name</span> <span class="o">=</span> <span class="s">'ConvLayer2'</span>
<span class="n">weights_conv2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">bias_conv2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>And then I manually do the convolution (same previous dog picture) using Scipy <a target="_blank" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"><code class="highlighter-rouge">convolve2d</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">input_image</span> <span class="o">=</span> <span class="n">testData</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convolve2d</span><span class="p">(</span><span class="n">input_image</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                             <span class="n">weights_conv1</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                             <span class="n">boundary</span><span class="o">=</span><span class="s">'fill'</span><span class="p">))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div>

<figure>
  <img src="http://localhost:4000/public/images/dog_scipy_conv2d.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Result of Scipy convolve2d.</figcaption>
</figure>

<p>Followed by the cross-correlation using Scipy <a target="_blank" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate2d.html"><code class="highlighter-rouge">correlate2d</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">input_image</span> <span class="o">=</span> <span class="n">testData</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correlate2d</span><span class="p">(</span><span class="n">input_image</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                              <span class="n">weights_conv1</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div>

<figure>
  <img src="http://localhost:4000/public/images/dog_scipy_corr2d.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Result of Scipy correlate2d.</figcaption>
</figure>

<p>In the end I do a convolution using a cross-correlation (remember the <em>τ</em> and <em>t</em> signal change):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">input_image</span> <span class="o">=</span> <span class="n">testData</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correlate2d</span><span class="p">(</span><span class="n">input_image</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                            <span class="n">numpy</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">flipud</span><span class="p">(</span><span class="n">weights_conv1</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">])),</span> <span class="n">mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div>

<figure>
  <img src="http://localhost:4000/public/images/dog_scipy_corr2d_flipped.png?style=centerme" alt="dogs-vs-cats">
  <figcaption>Result of Scipy correlate2d, but flipping things (look again the convolution2d result).</figcaption>
</figure>

<p>That’s it, all done!</p>

<p>Lastly, let’s see what we have achieved:</p>

<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Undoubtedly convince ourselves <a target="_blank" href="https://keras.io/">Keras</a> is cool!</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Create our first very own <strong>Convolutional</strong> Neural Network.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Understand there’s no magic! (but if we just had <a target="_blank" href="https://en.wikipedia.org/wiki/Infinite_monkey_theorem">enough monkeys</a>…)</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Get a job on Facebook… ok, we will need a lot more to impress <a target="_blank" href="http://yann.lecun.com/">Monsieur Lecun</a>, but this is a starting point <img class="emoji" title=":bowtie:" alt=":bowtie:" src="https://assets-cdn.github.com/images/icons/emoji/bowtie.png" height="20" width="20">.</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>And, finally, enjoy our time while doing all the above things!</li>
</ul>

<p>I was almost forgetting, we can tick this box from the first <a target="_blank" href="http://localhost:4000/deep_learning/2017/01/29/easy-peasy_deep_learning/">post</a>:</p>

<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Show off by modifying the previous example using a convolutional layer.</li>
</ul>

<p>Alright, we still need to do a lot more to get that Facebook job position… but in <a target="_blank" href="https://en.wikipedia.org/wiki/Gaucho">Southern Brazil</a>, we have a saying <em>“Não tá morto quem peleia”</em> that I would translate as <em>“If you can still fight, the battle is not over”</em> <img class="emoji" title=":smile:" alt=":smile:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">.</p>

<p>As promised, <a target="_blank" href="http://nbviewer.jupyter.org/github/ricardodeazambuja/keras-adventures/blob/master/Dogs_vs_Cats/Keras%20Cats%20and%20Dogs%20-%20convolutional%20deep%20net%20(not%20so%20deep)%20-%20Final.ipynb">here</a> you can visualize (or download) a <a target="_blank" href="https://ipython.org/notebook.html">Jupyter (IPython) notebook</a> with all the source code and something else.</p>

<p>And that’s all folks! I hope you enjoyed our short Keras adventure. Cheers!</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      
    
      
    
      
        
          
            <li>
              <h3>
              <a class="" href="target=%22_self%22">
                
              </a>
             </h3>
            </li>
          
        
          
            <li>
              <h3>
              <a class="" href="/deep_learning/2017/03/05/easy-peasy_conv_deep_learning_two/" target="_self">
                Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 2
              </a>
             </h3>
            </li>
          
            <li>
              <h3>
              <a class="" href="/deep_learning/2017/02/12/easy-peasy_deep_learning_one_and_a_half/" target="_self">
                Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1½
              </a>
             </h3>
            </li>
          
            <li>
              <h3>
              <a class="" href="/deep_learning/2017/01/29/easy-peasy_deep_learning/" target="_self">
                Easy-peasy Deep Learning and Convolutional Networks with Keras - Part 1
              </a>
             </h3>
            </li>
          
        
      
    
      
    
      
    
      
    
      
    
  </ul>
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/linux/2017/02/28/Nohup_magic/" target="_self">Older</a>
  
  
    <a class="pagination-item newer" href="/windows/2017/04/06/recover_windows_password/" target="_self">Newer</a>
  
</div>





<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/deep_learning/2017/03/05/easy-peasy_conv_deep_learning_two"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//ricardos-place.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a target="_blank" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>



      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
